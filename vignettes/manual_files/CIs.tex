

%For debugging, the file is set up to be Sweaved on its own.
% Sweave("CIs.Rnw")
\chapter{Standard errors, confidence intervals and prediction intervals for MARSS models}
\label{chap:cis}
\chaptermark{Computing CIs and PIs}
%Add footnote with instructions for getting code
\blfootnote{Type \texttt{RShowDoc("Chapter\_CIs.R",package="MARSS")} at the R command line to open a file with all the code for this chapter.}


This chapter discusses standard errors, confidence intervals and prediction intervals for the states and observations in a MARSS model and shows you how to get these outputs with the MARSS functions.  We will start by illustrating these concepts with a simple linear regression and then discuss them in the more complicated state-space context.  When discussing confidence interval and prediction intervals, it is important to recognize that they concern properties of data we have not collected rather than data we have collected.  When we write of the expected value of $\tilde{y}$ (new data) conditioned on the observed data, these two datasets are different.  We do not need to calculate the expected value of data we have collected; the expected value is simply its value.  To distinguish these two types of data, the data collected is called $y$ and new data (and whose expected value we are interested in) is called $\tilde{y}$.

For this chapter, we will use a data set on the date of first spring flight observed in two butterfly species in the California Sierra from 1972 to 2002\footnote{Collected by Dr. A. M. Shapiro at UC Davis.  http://butterfly.ucdavis.edu/}.
We will regress this data against the average maximum daily temperature in February (Figure \ref{fig:dat}).
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]\label{fig:dat}
\begin{center}
\includegraphics{../figures/CIs--Cs_000_fig0}
\end{center}
\caption{First flight data for Common Skippers at field sites in Northern California 1972-2002.  Left) The day of first observed spring flight ($y$) relative to the mean versus the average maximum daily temperature ($x$) recorded at the Willow Slough site in January to March.  On the $y$ axis, 0 indicates the average observed day of first flight from 1972 to 2002; 20 means 20 days after the average and -20 means before.  The blue dots are the observed data and the red dots are the 'hypothetical' data generated from the fitted relationship: $y=\alpha+\beta x + e$ where $e \sim N(0,\sigma)$. The point marked with an open circle is 1983 and was removed as an outlier year. Right) Histogram of average maximum winter temperatures in the 30-year butterfly dataset.}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We will assume that these data are a random sample from this generating process:
\begin{equation}\label{eqn:genmodel}
y = \alpha + \beta x + e, \quad\quad e \sim N(0,\sigma)
\end{equation}
That is, that the day of first spring flight relative to the mean is a linear function of the average winter maximum temperature. We will assume that $\alpha$ = 299.655, $\beta$= -5.064, and $\sigma$=9.492 based on the 1972-2002 data.  The red dots in Figure \ref{fig:dat} show hypothetical data generated from this model with the observed data shown in blue. The red line is the expected value of data generated with Equation \ref{eqn:genmodel} at each $x$ value on the $x$-axis.  We will denote this expected value as $E[\tilde{y}|x]$.

\section{Definition and properties of confidence intervals }

A 95\% confidence interval on some test statistic is an interval constructed in such a way that for 95\% of the possible new data sets, the confidence interval includes (or covers) the true value of the statistic.  We will be computing
confidence intervals for $E[\tilde{y}|x=58]$, the value of the red regression line at $x=58$, based on a sample (the observed data) from the generating model (Equation \ref{eqn:genmodel}).

Let's say we observe 10 data points from the generating model (Equation \ref{eqn.genmodel}). We will denote these observations $\yy_j$ and through this chapter $j$ is referring to this particular observed set of data.
To generate a $\yy_j$ for this chapter, we will set the random seed to 123, and then will generate both $y$ (first flight day relative to mean) and $x$ (average max winter temperature):
\begin{Schunk}
\begin{Sinput}
 set.seed(123)
 nsamp=10 #sample size
 x.j = runif(nsamp, min(dat$x), max(dat$x))
 y.j =  alpha + beta*x.j + rnorm(nsamp,0,sigma)
 dat.j = data.frame(x=x.j, y=y.j)
 #we will use the fit to the j-data throughout the chapter
 fit.j = lm(y~x, data=dat.j)
 df.j = fit.j$df.residual
 alpha.j = coef(fit.j)[1]
 beta.j = coef(fit.j)[2]
 sigma.j = sqrt(sum(fit.j$residual^2)/df.j) #unbiased not MLE
\end{Sinput}
\end{Schunk}
There are other possible $y$ data we could have collected with the same $x$ values, say different years with the same set of temperatures or same years but a different schedule of site visits.  These hypothetical samples will be denoted $\tilde{\yy}_j$; the the $j$ subscript reminds us that the $x$ values are $\xx_j$.

Figure \ref{fig:CIs.basics}c shows a regression line fit to $\yy_j$. The value of this regression at $x=58$ is our estimate of the expected value of new or hypothetical data generated at the value $x=58$, denoted $\widehat{E[\tilde{y}|x=58]}$. Our estimate of $E[\tilde{y}|x=58]$ is $\hat{\alpha}_j + \hat{\beta}_j 58$.  We can construct a confidence interval for $E[\tilde{y}|x=58]$. 

This 95\% confidence interval is an interval constructed in such a way that for 95\% of the possible random samples of 10, the confidence interval includes (or covers) the true expected value: $\alpha + \beta 58$.  What does this mean?  We can imagine other samples of 10 and other regression lines that we would compute with those samples. Here is  code to create a large number of $\alpha$, $\beta$ and $\sigma^2$ estimates from samples of 10:
\begin{Schunk}
\begin{Sinput}
 nsim = 5000
 i.results=matrix(NA,nsim,3)
 for(i in 1:nsim){
   x = runif(nsamp, min(dat$x), max(dat$x))
   y = alpha + beta*x + rnorm(nsamp,0,sigma)
   dat.i=data.frame(x=x, y=y)
   fit.i=lm(y~x, data=dat.i)
   i.results[i,]=c(coef(fit.i), 
                   sqrt(sum(fit.i$residual^2)/fit.i$df.residual))
 }
\end{Sinput}
\end{Schunk}
For each sample, we could compute $E[\tilde{y}|x=58]=\hat{\alpha}+\hat{\beta} 58$ and we could construct a confidence interval for it just like in Figure \ref{fig:CIs.basics}c.  If the confidence interval is constructed properly, for 95\% of the regression lines in Figure \ref{fig:CIs.basics}b, the interval will include $\alpha + \beta 58$.  Figure \ref{fig:CIs.basics}d shows that this is the case for the interval constructed as in panel b. The $y$-axis is the fraction of CIs that cover $\alpha + \beta 58$ minus the correct fraction (99\%, 95\% or 75\%).

% \subsection{The nature of the different samples of 10}
% 
% Before proceeding, we should clarify how the different samples or observations of 10 are generated---what is the nature of the data that produces the distribution of regression lines in Figure \ref{fig:CIs.basics}b.  One possibility is that we collected data for years with particular maximum winter temperatures.  In this scenario, the values on the $x$-axis were a choice on our part.  Even if we don't set the value of the $x$, we could effectively be doing that if we said ahead of time that we were going to collect data  in 10 specific years.  In that scenario, if we imagine collecting a different set of data, then we should fix the $x$ for $\tilde{y}$ to be that in our observed data.  Alternatively, we might imagine that the $x$ are random and that if we were to collect a different data set, the $x$ would be different.  
% 
% Knowing how the predictor variables were choosen (randomly or by design) is important because CIs that are correct for one case, won't necessarily be correct for the other case.  This mainly comes up when constructing confindence intervals via bootstraping.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]
\begin{center}
\includegraphics{../figures/CIs--Cs_001_fig1}
\end{center}
\caption{Properties of confidence intervals}
\label{fig:CIs.basics}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\subsection{Logic behind the construction of confidence intervals}

Figure \ref{fig:cartoon.cis} walks through the logic behind algorithms for construction of confidence intervals.  There are two approaches you could take based on the distribution of regression lines from fixed $x$ values or from random $x$ values.  The former is the typical way to construct CIs but the later arises in some types of bootstrapping.  

Using the left-hand strategy (panels a-c), we start by thinking about a particular set of $x$ values, the $\xx_j$ which are marked as green lines in panel a).  At those values, we can imagine a sets of $y$ generated with the model:
$$\yy = \alpha + \beta \xx_j + \ee$$
where the $e$ in $\ee$ are drawn from a Normal distribution with variance $\sigma^2$.  We could then estimate the regression lines from the $\yy$ datasets.  These are shown by the grey lines in panel a).  We could construct a confidence interval at $x=58$ using the 95\% range of values of the grey lines at $x=58$ (panel b).  If we were to center that blue line on each of regression lines in panel a), it would cover the red line 95\% of the time.  

The strategy in the right-hand panels is similar except the regression lines are generated from sets of $x$ that are drawn randomly from the possible set of $x$.  In the 30-year butterfly dataset, the $x$ have bi-modal distribution (Figure \ref{fig:dat}).  We  imagine drawing 10 $x$ randomly from that distribution and generate $\yy$ with
$$\yy = \alpha + \beta \xx + \ee$$
that would lead to a different distribution of regression lines shown in panel d.  We would construct the blue line from the distribution of lines in panel d and use that for the CI for any set of 10 $x$ we might draw.  

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]
\begin{center}
\includegraphics{../figures/CIs--Cs_002_fig_cartoon}
\end{center}
\caption{Two approaches to construction CIs that will cover the red line (the true regression line) for 95\% of any sample of 10.  On the left side, we hold the $x$ values constant and on the right, we chose them randomly from the possible sets of $x$ values.  In panel a) we show the distribution of regression lines for a particular set of $x$ values, $\xx_j$, shown by the green lines. The grey lines are random regression lines from samples generated from $y=\alpha + \beta \xx_j + \ee$, where the $e$ in $\ee$ are drawn from a Normal distribution with variance $\sigma^2$.  In panel b) the blue line is a 95\% CI constructed from the 95\% range of the grey line values at $x=58$. In panel c) we show one particular regression line (from the grey lines in panels a and b) from one particular set of $y$ (the crosses) at $\xx_j$.  We center the blue line from panel b) on the black regression line value at $x=58$.  For 95\% of the regression lines (the grey lines) in panel a), the blue line centered in this way will cover the red line.  This works for any set of $\xx$ values and thus this approach will properly define a CI for any $\xx$.  On the right side the strategy is similar, but instead of generating regressions at one set of $\xx_j$, the regressions are from $\xx$ drawn randomly from the possible sets of $\xx$.}
\label{fig:cartoon.cis}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The trick is to come up with a strategy for estimating the blue line---that is the distribution of regression lines in panels a) or d).  The blue line is determined by the generating model. The left panel of Figure \ref{fig:distparam} shows the true bivariate distribution of $\hat{\alpha}$ and $\hat{\beta}$. We need to know the shape of this, but we do not need to know the true $\alpha$ and $\beta$ that determine the location of the maximum.  That's because the shape determines the length of the blue line and that's all we need to construct our CI.  The different strategies for constructing CIs are based on estimating the shape of the distribution on the left from the data.  The right hand panel illustrates this.  This is the distribution of $\hat{\alpha}$ and $\hat{\beta}$ estimates from data sets generated from parametric bootstaps from one set of data\footnote{Fit model to observed data. Use that model to generate data.  Fit model to bootstrap data to get parameter estimates. Repeat.}.  You can see that the shape is similar to that on the left. All CI strategies are based on this idea of using the observed data to estimate the shape of the true distribution of $\hat{\alpha}$ and $\hat{\beta}$.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]\label{fig:distparam}
\begin{center}
\includegraphics{../figures/CIs--Cs_001_dist_of_param}
\end{center}
\caption{True distribution of $\hat{\alpha}$ and $\hat{\beta}$ from random samples of 10 from the true generating model (left) versus the distribution from fitting a model to observed data and then estimating $\alpha$ and $\beta$ from bootstrap data sets created with the model estimated from the observed data.}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



\section{Different ways to compute confidence intervals}

We cover five approaches for constructing confidence intervals.  The first four use a parametric model: the linear model with Gaussian independent errors.  The first of these is the analytical CI using a known $\sigma^2$, the second is the analytical CI using an estimate of $\sigma^2$, the third is a parametric bootstrap, and the fourth uses a numerically estimated Hessian matrix.  The fifth approach uses resampling from the data. 

All the above methods are asymptotically correct, as $n$ gets large.  This means as sample size gets large, all methods will produce $x$-CIs (e.g. 95\% CIs) that cover the true value $x$\% of the time.  

\subsection{Analytical construction of CIs using a known $\sigma$}

We constructed the blue line in panel b) of Figure \ref{fig:cartoon.cis} by simulation.  The code is shown below.  Notice that we keep the $x$ values constant and simulate new residual errors.
\begin{Schunk}
\begin{Sinput}
 nsim=5000
 ij.results=matrix(NA,nsim,2)
 for(i in 1:nsim){
   y.ij=alpha+beta*x.j+rnorm(nsamp,0,sigma)
   fit.ij=lm(y.ij ~ x.j)
   ij.results[i,]=coef(fit.ij)
 }
 x1=58
 CI=quantile(ij.results[,1]+ij.results[,2]*x1,probs=c(0.025,.975))
\end{Sinput}
\end{Schunk}
\verb@CI@ in the code is the blue line in panel b) in Figure \ref{fig:cartoon.cis}.

However, we do not need to simulate for this problem because there is an analytical solution.
The asymptotic\footnote{meaning large $n$.} distribution of $\hat{\alpha}$ and $\hat{\beta}$ values that define the grey lines in panels a) and b) in Figure \ref{fig:cartoon.cis} is a multivariate normal distribution:
\begin{equation}
\begin{gathered}
\begin{bmatrix}\hat{\alpha}\\\hat{\beta}\end{bmatrix} \sim \MVN\left(
\ttheta, \Sigma \right)\\
\ttheta = \begin{bmatrix}\alpha\\ \beta\end{bmatrix}\text{ and } \Sigma = \II(\ttheta)^{-1}
\end{gathered}
\label{eqn:dist.mls}
\end{equation}
where $\II(\ttheta)$ the Fisher information matrix.  The Fisher information matrix is the second derivative of the negative log-likelihood function:
$$\II(\ttheta)=\begin{bmatrix}
\frac{\partial^2-\log L}{\partial\alpha \partial\alpha}&\frac{\partial^2-\log L}{\partial\alpha \partial\beta}\\
\frac{\partial^2 -\log L}{\partial\alpha \partial\beta}&\frac{\partial^2 -\log L}{\partial\beta \partial\beta}
\end{bmatrix}$$

For our linear regression model with Gaussian errors, the Fisher information matrix has a simple equation: 
$$\II(\ttheta)=\frac{1}{\sigma^2}\XX_j^\top\XX_j$$
where $\XX_j$ is a 2 column matrix with 1s in column 1 and the predictor variables (the $\xx_j$) in column 2.  The asymptotic variance-covariance matrix of the MLEs is the inverse of this, i.e. $\sigma^2( \XX_j^\top\XX_j)^{-1}$. Thus for our $\xx_j$ in panel a (the green lines), the estimated $\alpha$'s and $\beta$'s will be normally distributed with a variance of
\begin{Schunk}
\begin{Sinput}
 Xj=cbind(alpha=1,beta=x.j)
 analytical.Sigma=sigma^2*solve(t(Xj)%*%Xj)
 analytical.Sigma
\end{Sinput}
\begin{Soutput}
          alpha        beta
alpha 9105.7640 -150.517899
beta  -150.5179    2.490519
\end{Soutput}
\end{Schunk}

We can use the variance-covariance matrix of the MLEs, $\Sigma$, to compute an interval around $\alpha+58 \beta$ that contains 95\% of the grey lines at $x=58$.  We know that
\begin{equation}
\var(\hat{\alpha} + 58 \hat{\beta}) = \var(\XX \hat{\ttheta}) =  \XX \var(\hat{\ttheta}) \XX^\top = \XX  \Sigma \XX^\top
\end{equation}
where $\XX$ is a matrix with 1 in column 1 and 58 in column 2 (58 is where we are construction the CI), , $\Sigma=\sigma^2 (\XX_j^\top\XX_j)^{-1}$, $\XX_j$ is a matrix with 1s in column 1 and $\xx_j$ in column 2,  and $\hat{\ttheta}=\begin{bmatrix}\hat{\alpha}\\ \hat{\beta}\end{bmatrix}$.
An interval that contains 95\% of these is
$$ \XX \ttheta + z^{.05/2} \sqrt{\XX\Sigma\XX^\top}= \XX \ttheta + 1.96 \sqrt{\XX\Sigma\XX^\top}$$
where $z^{.05/2}$ is the quantile of the unit Normal at $.05/2$.

Here is code to compute the analytical 95\% interval of the regression lines at $x=58$:
\begin{Schunk}
\begin{Sinput}
 Xj = cbind(1, dat.j$x)
 XjXj.inv = solve(t(Xj)%*%Xj)
 Sigma = sigma^2*XjXj.inv
 theta = rbind(alpha, beta)
 x=58; X = cbind(1, x)
 #the analytical CI
 X%*%theta + qnorm(c(0.025,.975)) * sqrt(X%*%Sigma%*%t(X))
\end{Sinput}
\begin{Soutput}
[1] -3.604446 15.516128
\end{Soutput}
\end{Schunk}
We can compare this to the real distribution of the regression lines at $x=58$:
\begin{Schunk}
\begin{Sinput}
 quantile(ij.results[,1]+ij.results[,2]*x, probs=c(0.025, 0.975))
\end{Sinput}
\begin{Soutput}
     2.5%     97.5% 
-3.487706 15.664808 
\end{Soutput}
\end{Schunk}
They are pretty close even though $n$ is quite small and the asymptotic solution is an approximation.

This gives us a way to calculate confidence intervals for $\E(\tilde{y}|x=58)$ if we know the true $\sigma$. The width of the CI is specified as above but we center it on $\XX \hat{\ttheta}$ which is our estimate of $\E(\tilde{y}|x=58)$:
$$\XX \hat{\ttheta}  + z^{.05/2} \sqrt{\XX\Sigma\XX^\top}$$
where $\Sigma=\sigma^2(\XX_j^\top\XX_j)^{-1}$.

The problem is we don't know $\sigma$; we only have an estimate of $\sigma$. 

\subsection{Analytical construction of CIs using an estimate of $\sigma$}

The parametric approach gives us a simple equation for $\Sigma$: $\sigma^2( \XX_j^\top\XX_j)^{-1}$.  Why not just use that with an estimate of $\sigma$? So why not use the \emph{observed} Fisher information matrix
$$ \II(\hat{\ttheta})=\frac{1}{\hat{\sigma}^2}(\XX_j^\top\XX_j)$$
in Equation \ref{eqn:dist.mls}.  The problem is the distribution of $\tilde{y}_j$ conditioned on $\sigma$ (known) is Normal but the distribution of $\tilde{y}_j$ conditioned on an estimate of $\sigma$ has a t-distribution. For large $n$, approximating a t-distribution by a Normal distribution is not too bad, but for small $n$ (like 10), it will lead to overly narrow CIs (too low coverage).

The corrected CIs are
\begin{equation}
\XX \hat{\theta}  + t^{.05/2}_{d} \sqrt{\XX\hat{\Sigma}\XX^\top}
\label{eqn:analytical.CIs}
\end{equation}
where $t^{.05/2}_{d}$ is the t-distribution quantile at 0.025 with the degrees of freedom for the $\sigma$ estimation (in our case $d=n-2$) and $\hat{\Sigma}=\hat{\sigma}^2 (\XX_j^\top\XX_j)^{-1}$. The R code to compute this is
\begin{Schunk}
\begin{Sinput}
 Xj = cbind(1, x.j)
 XjXj.inv = solve(t(Xj)%*%Xj)
 Sigma.j = sigma.j^2*XjXj.inv
 theta.j = matrix(c(alpha.j,beta.j), ncol=1)
 fit.df = fit.j$df.residual
 #Compute CI at x=58
 x=58; X = cbind(1, x)
 EyX = X%*%theta.j
 CI = EyX + qt(c(0.025,.975), df=fit.df) * sqrt(X%*%Sigma.j%*%t(X))
 correct.ci.j = c(fit=EyX, lwr=CI[1], upr=CI[2])
\end{Sinput}
\end{Schunk}
It is this confidence interval that R's \verb@predict@ function returns:
\begin{Schunk}
\begin{Sinput}
 correct.ci.j
\end{Sinput}
\begin{Soutput}
     fit      lwr      upr 
11.71982  1.99248 21.44715 
\end{Soutput}
\begin{Sinput}
 predict(lm(y~x, data=dat.j), new=data.frame(x=x),interval="confidence")
\end{Sinput}
\begin{Soutput}
       fit     lwr      upr
1 11.71982 1.99248 21.44715
\end{Soutput}
\end{Schunk}

We can simulate to show that analytical CIs with true $\sigma$ are correct and those with estimated $\sigma$ have under-coverage.  We simulate $y$'s at the $\xx_j$ shown by the green lines in Figure \ref{fig:cartoon.cis} and compute CIs at $x=58$ using true or estimated $\sigma$ and then using the correction.
Then we will see if these CIs cover the red line 95\% of the time (or not).

\begin{Schunk}
\begin{Sinput}
 Xj = cbind(1, x.j)
 XjXj.inv = solve(t(Xj)%*%Xj)
 true.Sigma = sigma^2*XjXj.inv
 #we are going to compute CI at x=58
 x=58; X = cbind(1, x)
 #holders
 nsim=5000
 i.CIs.bad=i.CIs.true=i.CIs.corr=matrix(NA,nsim,2)
 for(i in 1:nsim){
   tilde.y=alpha+beta*x.j+rnorm(nsamp,0,sigma)
   fit.i=lm(tilde.y ~ x.j)
   hat.theta=matrix(coef(fit.i),ncol=1)
   hat.sigma = sqrt(sum(fit.i$residual^2)/fit.i$df.residual)
   hat.Sigma = hat.sigma^2*XjXj.inv
   meanCI = X%*%hat.theta
   normaldist = qnorm(c(0.025,0.975))
   tdist = qt(c(0.025,0.975),df=fit.i$df.residual)
   #CI using asymptotic equation and estimated Sigma
   i.CIs.bad[i,] = meanCI + normaldist * sqrt(X%*%hat.Sigma%*%t(X))
   #CI using asymptotic equation and true Sigma
   i.CIs.true[i,] = meanCI + normaldist * sqrt(X%*%true.Sigma%*%t(X))
   #Corrected CI using t-distribution
   i.CIs.corr[i,] = meanCI + tdist * sqrt(X%*%hat.Sigma%*%t(X))
 }
\end{Sinput}
\end{Schunk}

The true value (red line) at $x=58$ is $\alpha+ 58 \beta$.  The correct CI using the true $\sigma$ has the correct coverage:
\begin{Schunk}
\begin{Sinput}
 x=58
 true.val = alpha+beta*x
 100*sum(i.CIs.true[,1]<true.val & i.CIs.true[,2]>true.val)/nsim
\end{Sinput}
\begin{Soutput}
[1] 94.96
\end{Soutput}
\end{Schunk}
But the CIs using the estimated $\sigma^2$ are too narrow.  They have low coverage (less than 95\%):
\begin{Schunk}
\begin{Sinput}
 100*sum(i.CIs.bad[,1]<true.val & i.CIs.bad[,2]>true.val)/nsim
\end{Sinput}
\begin{Soutput}
[1] 91.54
\end{Soutput}
\end{Schunk}
If we use the t-distribution's 95\% intervals to compute our CIs, the coverage is correct again:
\begin{Schunk}
\begin{Sinput}
 100*sum(i.CIs.corr[,1]<true.val & i.CIs.corr[,2]>true.val)/nsim
\end{Sinput}
\begin{Soutput}
[1] 94.66
\end{Soutput}
\end{Schunk}

This illustrates the problem of using an estimate of $\sigma$ instead of the true value.  This same problem will arise when we look at other approaches to computing confidence intervals.


\subsection{Constructing confidence intervals using a numerically estimated information matrix}

To construct the analytical CIs, we estimated the distribution of $\alpha + 58 \beta$ using an estimate of the distribution of $\hat{\alpha}$ and $\hat{\beta}$ based on the observed Fisher information matrix: $\hat{\Sigma}= \II(\hat{\ttheta})^{-1}$.  We have an analytical solution for $\II(\hat{\ttheta})^{-1}$, but we could also use R to generate a numerical estimate of the information matrix. This doesn't make much sense here since we have an analytical solution, but it is useful when we do not know or have the analytical solution. 

Another term for the observed Fisher information matrix is the Hessian of the negative log-likelihood function at $\hat{\ttheta}$. There are a number of R functions that will estimate the Hessian of a function.  We will use \verb@optim()@.  First we define function to return the negative log-likelihood for our model, a linear regression with Gaussian errors.  

\begin{Schunk}
\begin{Sinput}
 # Define the log likelihood function for a linear regression
 # parm is the alpha, beta, sigma vector
 NLL <- function(parm,  dat=NULL){
   #parm is alpha, beta, sigma
   resids = dat$y - dat$x * parm[2] - parm[1]
   dresids = suppressWarnings(dnorm(resids, 0, parm[3], log = TRUE))
   -sum(dresids)
 }
\end{Sinput}
\end{Schunk}

Then we can pass this function into \verb@optim()@ with \verb@hessian=TRUE@.  To work well, \verb@optim()@ needs good starting values.  We pass in really good ones, i.e. the output from \verb@lm()@.  Remember that $j$ here is referring to the observed $j$ sample of 10 (`the data').  $\alpha_j$ is the $\alpha$ estimate from that sample.
\begin{Schunk}
\begin{Sinput}
 start.pars = c(alpha.j, beta.j, sigma.j)
 ofit.j=optim(start.pars, NLL, dat=dat.j, hessian=TRUE)
 parSigma = solve(ofit.j$hessian)[1:2,1:2]
 parMean = ofit.j$par[1:2]
\end{Sinput}
\end{Schunk}
This will output the observed Fisher information matrix at the MLEs.  The $\sigma$ that \verb@lm()@ uses is different; it is the unbiased estimate.  Thus the observed Fisher information matrix output here is different because it is using $\sigma_{MLE}$.  We could change our code to use the unbiased estimate, but often one computes the Hessian at the MLEs, including the MLE of $\sigma$. 

We can then use the Hessian to compute CIs:
\begin{Schunk}
\begin{Sinput}
 X = cbind(1, 58)
 EyX = X%*%parMean
 hessian.cis = c(EyX, EyX + qnorm(c(0.025,.975))*sqrt(X%*%parSigma%*%t(X)))
\end{Sinput}
\end{Schunk}

Because we use the estimated variance instead of the true variance, the confidence intervals from the numerical estimate of the observed Fisher information matrix should also be too narrow:
\begin{Schunk}
\begin{Sinput}
 rbind(hessian=hessian.cis, correct=correct.ci.j) 
\end{Sinput}
\begin{Soutput}
             fit      lwr      upr
hessian 11.71696 4.321769 19.11216
correct 11.71982 1.992480 21.44715
\end{Soutput}
\end{Schunk}

Though not necessary here, often one computes CIs by simulating from the estimated $\Sigma$, generating a large number of estimates of the metric of interest, and then using the quantiles of that.  Section \ref{sec:funcs} includes a functions \verb@hessian.boot@ and \verb@hessian.boot.cis@ to generate CIs using parameter estimates generated from an estimated information matrix.  

\subsection{Constructing CIs via bootstrapping}\label{subsec:bootstrap}

The basic idea behind a bootstrap CI is that the data are used to generate new data sets (bootstrap data sets) from which parameters are estimated to give a large set of bootstrap parameter estimates and thus regression lines.  On average, the variance-covariance matrix of the bootstrapped parameter estimates will be close to the variance-covariance matrix of the MLE parameter estimates ($\Sigma$ for our example).  Thus the bootstrapped parameter estimates can used to generate CIs.

The procedure is simple.  First you generate a large number of bootstrapped data sets, then estimate the model parameters from each data set to get the bootstrap parameter estimates.  For each set of bootstrapped parameter estimates, compute the metric of interest.  We are interested in CIs for the fitted value at $x$, so we compute $\hat{\alpha}_b + \hat{\beta}_b x$ for each bootstrap.  The 95\% quantiles of the $\hat{\alpha}_b + \hat{\beta}_b x$ define the bootstrap CIs. Here is a function to do this:
\begin{Schunk}
\begin{Sinput}
 #takes a set of boot parameters and makes CIs from them at x
 boot.CI=function(boot.params, x, alp=0.05){
   CIs=apply(
     boot.params[,c("alpha","beta"),drop=FALSE]%*%rbind(1,x),
     2,quantile, c(0.5, alp/2, 1-alp/2) )
   colnames(CIs)=x
   t(CIs) #to look like predict output
 }
\end{Sinput}
\end{Schunk}

The are a different ways to generate the bootstrap data sets.  We cover parametric bootstrapping, residuals resampling, and data resampling.

\subsection{Constructing CIs via parametric bootstrapping }

In a parametric bootstrap, the estimated model is used to generate bootstrapped data, and the parameters are estimated from that bootstrapped data.  The bootstrapped parameter estimates are then used to construct CIs.  It is the same idea as a traditional bootstrap, but we are not sampling from the data to generate new bootstrap data sets but rather using the estimated model to generate new data.  This is similar to the analytical CI approach and CIs from an estimated Hessian, in that one uses the model and the parameter estimates to estimate the distribution of parameter estimates.

Why use a parametric bootstrap instead of the analytical CIs?  You use it when you want CIs based on your parametric model, but you do not know the analytical solution of $\Sigma$ or you want to approximate the small $n$ distribution rather than use the large $n$ approximation.  Why not use  an estimate of the observed Fisher information matrix (Hessian)? You do not want to use the large $n$ approximation or estimation of the Hessian is unstable.

To generate bootstrap parameter estimates via a parametric bootstrap, we generate data from our estimated model:
$\yy_b= \hat{\alpha}+\hat{\beta} \xx + \ee$
where each $e$ in  $\ee$ is drawn from a Normal distribution with mean 0 and variance $\hat\sigma^2$.  From each $\yy_b$, we estimate $\alpha$ and $\beta$ as usual, e.g. using \verb@lm@, and construct the CIs using the set of bootstrap estimates.  

Section \ref{sec:funcs} shows functions \verb@parametric.boot@ and \verb@parametric.boot.cis@ to do this.  It takes the original data, gets the MLEs, and then uses those to generate data and a set of bootstrapped parameter estimates.  Notice that the function holds the $x$ values equal to our observed values when simulating new data.  That's rather important.


Parametric bootstrapping uses the estimated $\sigma$ to generate new data.  Thus, as you would expect, the confidence intervals should be too narrow:
\begin{Schunk}
\begin{Sinput}
 rbind(
   parametric=parametric.boot.cis(dat.j, x=58)[1,],
   correct=correct.ci.j
 )
\end{Sinput}
\begin{Soutput}
                50%     2.5%    97.5%
parametric 11.61505 4.119191 19.58689
correct    11.71982 1.992480 21.44715
\end{Soutput}
\end{Schunk}
However the problem diminishes as sample size increases; 10 is a very small sample size.  We will address how to correct this bias after covering all the bootstrapping approaches.

We can compare the bivariate distributions of parameter estimates from a parametric bootstrap to parameters drawn from the estimated parameter distribution using the numerically estimated Hessian matrix at the MLEs (Figure \ref{fig:dist.of.ests.est.vs.true}).  They should be very similar.  The only difference (besides the parametric bootstrap using simulation) is that the Hessian approach is based on a large $n$ approximation and our parametric bootstrap used the unbiased estimate of $\sigma$.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]
\begin{center}
\includegraphics{../figures/CIs--Cs_021_fig_biv_comparison}
\end{center}
\caption{Comparison of the distribution of $\alpha$ and $\beta$ estimates from parametrically bootstrapping using the data $\yy_j$ to the distribution from the Hessian of the log-likelihood function at the MLE values.}
\label{fig:dist.of.ests.est.vs.true}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\subsection{Constructing CIs via resampling from the residuals}

The analytical and parametric bootstrap CIs are based on assuming that the residual errors can be described by a particular statistical distribution. In our example we use a Normal distribution. Sampling from the residuals allows us to compute CIs when we are unwilling to make a specific assumption about the distribution but are willing to treat the structure of the data as known: $\yy=\alpha+\beta \xx+\ee$. Instead of assuming the $\ee$ come from a specific distribution, we generate $\ee$ for our bootstrap data by sampling with replacement from the residual errors from the fit to the original data. Section \ref{sec:funcs} shows functions to do this.


Once a large number of bootstrap data sets are generated, the construction of confidence intervals proceeds as for the parametric bootstrap. The model is fit to each bootstrap dataset $\yy_b$ and $\alpha_b$ and $\beta_b$ are estimated. Those parameters are then used to compute CIs.  

However, once again we used the variance in the observed residuals to generate samples of residuals.  So, again you would expect the confidence intervals should be too narrow.  And they are:
\begin{Schunk}
\begin{Sinput}
 rbind(
   residuals=residuals.boot.cis(dat.j, x=58)[1,],
   correct=correct.ci.j
 )
\end{Sinput}
\begin{Soutput}
               50%     2.5%    97.5%
residuals 11.99081 3.566076 18.43254
correct   11.71982 1.992480 21.44715
\end{Soutput}
\end{Schunk}

\subsection{Constructing confidence intervals via resampling the data}

The last approach we will cover for creating bootstrap datasets is to sample, with replacement, from the data.  This approach follows the logic of panels d-f in Figure \ref{fig:cartoon.cis}.  We are trying to estimate, via resampling, the distribution of regression lines that we would see if we had many samples of data drawn from the 'universe' of possible $y$ and $x$.  

Note that in this approach, the predictor variables, the $x$, in your bootstrap data (your $\yy_b$) change from bootstrap to bootstrap.  That might be problem. The variance of your regressions depends on your $x$ values.  What if your $x$ values must be fixed---because you choose them to be a particular value, for example, as part of your experimental design.  Using anything other than the $x$ values you chose would give the wrong CIs.  In this case, resampling from the residuals would make more sense.

In other cases, the $x$ values you observed were random (you did not choose them). If your sample size is large enough, resampling from the data has some benefits.  Like resampling from the residuals, you are not assuming a specific distribution for the residuals, unlike the analytical CIs or parametric bootstrap CIs.  Unlike for sampling from the residuals, you also allow that the residual distribution could be different for different $x$ values.  For example, this approach would allow the residual variance to be smaller for large $x$ and larger for small $x$, say.  

Section \ref{sec:funcs} shows functions to create bootstrap data by resampling the data.  Once a large number of bootstrap data sets are generated, the CIs are created as usual.
As usual we are using the variance in the data as a proxy for the variance in the 'universe' of data and the CIs will tend to be too narrow.  However, this is countered by the fact that the $x$ values in our resamples will have lower spread (because we are resampling from the observed $x$) and this tends to cause the CIs to be larger than they should be.  This problem is severe for small samples, like $n=10$.
\begin{Schunk}
\begin{Sinput}
 rbind(
   resampling=resampling.boot.cis(dat.j, x=58)[1,],
   correct=correct.ci.j
 )
\end{Sinput}
\begin{Soutput}
                50%     2.5%    97.5%
resampling 11.74418 -3.16275 19.17828
correct    11.71982  1.99248 21.44715
\end{Soutput}
\end{Schunk}
But this problem diminishes as sample size increases:
\begin{Schunk}
\begin{Sinput}
 #create a larger sample
 nsamp=50
 x = runif(nsamp, min(dat$x), max(dat$x))
 y = alpha + beta*x + rnorm(nsamp,0,sigma)
 dat.big=data.frame(x=x, y=y)
 rbind(
   residuals=resampling.boot.cis(dat.big, x=x1)[1,],
   correct=predict(lm(y~x, data=dat.big), new=data.frame(x=58),interval="conf")
 )
\end{Sinput}
\begin{Soutput}
               50%     2.5%    97.5%
residuals 4.901254 1.666834 7.864881
1         4.920030 1.777711 8.062349
\end{Soutput}
\end{Schunk}

\subsection{Correcting under-coverage of CIs}

Ignoring the fact that we used an estimate of $\sigma$ (either explicitly or implicitly) rather than the true value leads to overly small CIs with under-coverage.  We saw this when the CIs from the parametric, non-parametric and Hessian bootstraps were compared to the correct analytical CIs coming from the \verb@predict@ function.  

How do we fix this? We need to estimate the correction factor, i.e how much to increase the width of our estimated CIs.  From the analytical CIs, we know that this correction factor is $t_{5\%/2, df}/z_{5\%/2}$ or in R \verb@qt(0.025, df=8)/qnorm(0.025)@ for our example with 10 data points and Gaussian errors.  We can estimate the correction factor via simulating from the estimated model.  The basic idea is to get a correct CI for one set of parameters, the estimated parameters, and then generate bootstrap data and estimate bootstrap CIs.  Then you estimate how small, on average, the bootstrap CIs are to the CI estimated from the observed data.  That mean bias is the correction factor.  Section \ref{sec:funcs} has a function to do this.


Here is the 
\begin{Schunk}
\begin{Sinput}
 #the correct adjustment:
 qt(c(0.025),df=fit.j$df.residual)/qnorm(c(0.025))
\end{Sinput}
\begin{Soutput}
[1] 1.176554
\end{Soutput}
\begin{Sinput}
 #adjustment computed via bootstrapping
 ci.adj(dat.j, 58, type="analytical")
\end{Sinput}
\begin{Soutput}
[1] 1.070423
\end{Soutput}
\end{Schunk}
Unfortunately computing the CI adjustment with bootstrapping takes a large number of bootstraps and is thus quite slow particular for the parametric and residuals bootstraps. Also it is an estimate of the bias.  The expected value of the estimate will be the correct bias, but for any one data set, the estimated bias will not be precisely correct.

Why not draw $\sigma$ from its bootstrapped distribution and use that in our CI construction? So instead of using $\hat{\alpha}_b$ and $\hat{\beta}_b$ only, we also use $\hat{\sigma}^2_b$. Wouldn't that properly account for the fact that we use an estimate of $\sigma$?  No, unfortunately it does not. 


\section{Prediction intervals on the fitted $\tilde{y}$}

The confidence intervals in Figure \ref{fig:CIs.1} are for $\alpha + \beta x$, or the expected value of $\tilde{y}$.  The prediction intervals show the (estimated) 95\% interval of $\tilde{y}$, not the expected value of $\tilde{y}$.  The 95\% prediction intervals should contain (or cover) 95\% of $\tilde{y}$ (mpg) observed for cars of weight $x$. Prediction intervals are wider than confidence intervals because prediction intervals predict the distribution of $\tilde{y}$ while confidence intervals predict the expected value of $\tilde{y}$.

Let's go back to our true relationship between mpg and weight (the red line in Figure \ref{fig:CIs.basics}a). This is the relationship for our 32 car dataset, but let's imagine it holds for all models of cars. Let's also say that the residual variance we see in Figure \ref{fig:CIs.basics}a (the difference between the dots and the red line) characterizes the variability in the relationship for all models of cars.  We could imagine many more data points (car models) around the red line, which we could generate like so:
\begin{Schunk}
\begin{Sinput}
 fit = lm(mpg~wt, data=mtcars)
 s2 = sum(fit$residual^2)/fit$df.residual
 alpha=coef(fit)[1]
 beta=coef(fit)[2]
 plot(mtcars$wt,mtcars$mpg,ylim=c(0,60),xlab="car weight",ylab="mpg")
 x = runif(1000,1,6)
 y = alpha+beta*x+rnorm(1000,0,sqrt(s2))
 points(x,y)
 abline(fit, lwd=2, col="red")
\end{Sinput}
\end{Schunk}

Then we could construct an interval at any $x$ (car weight) that would contain 95\% of the data points at that  $x$ (Figure \ref{fig:PIs}a).  For any new $\tilde{y}$, there would be a 95\% chance it would fall within our constructed interval.  Since our errors are Gaussian, this is just $\alpha+\beta x \pm 1.96\sigma$.  We could add these to our plot using:
\begin{Schunk}
\begin{Sinput}
 lines(c(0,10),alpha+beta*c(0,10)+1.96*sqrt(s2),col="red",lty=2,lwd=1)
 lines(c(0,10),alpha+beta*c(0,10)-1.96*sqrt(s2),col="red",lty=2,lwd=1)
\end{Sinput}
\end{Schunk}

However, we do not know $\alpha$, $\beta$ or $\sigma^2$, so we cannot construct this perfect prediction interval.  But we can devise a method of constructing a prediction interval such that 95\% of the constructed prediction intervals will cover new $\tilde{y}$.  This is the same idea as constructing confidence intervals except that now we are predicting $\tilde{y}$ instead of the expected value of $\tilde{y}$.

The analytical equation for the prediction intervals assumes the data come from the linear model with Gaussian errors and can be generated using the predict function \verb@predict@:
\begin{Schunk}
\begin{Sinput}
 npred=1000
 plot(mtcars$wt,mtcars$mpg,ylim=c(0,60),type="n",xlab="car weight",ylab="mpg")
 x = runif(npred,1,6)
 y = alpha+beta*x+rnorm(npred,0,sqrt(s2))
 points(x,y, col="grey")
 abline(fit, lwd=2, col="red")
 abline(fit.j, lwd=2)
 points(mtcars.j$wt, mtcars.j$mpg, pch=3)
 preds = predict(lm(mpg~wt,data=mtcars.j), newdata= data.frame(wt=pred.wt), interval="prediction")
 lines(pred.wt,preds[,2],lty=2)
 lines(pred.wt,preds[,3],lty=2)
\end{Sinput}
\end{Schunk}
Figure \ref{fig:CIs.2}b shows the prediction intervals computed using our $j$ sample (the crosses).  These are a bit wide and cover a little too much of the $\tilde{y}$ distribution (the grey circles).  The prediction interval is worse, meaning too much coverage, the farther we try to predict from the center of the prediction variable (car weight):
\begin{Schunk}
\begin{Sinput}
 #get the 95% pred intervals for each x
 preds = predict(fit.j, newdata= data.frame(wt=x), interval="prediction")
 #see how many y fall outside that
 1-sum(y>preds[,3] | y<preds[,2])/npred
 #see how many fall inside at different x values
 1-tapply(y>preds[,3] | y<preds[,2],cut(x,breaks=1:6),mean)
\end{Sinput}
\end{Schunk}

However that was just for one sample of 10 cars.  If we look at large number of 10 car samples, we see that on average the prediction interval does cover 95\% of the $\tilde{y}$:
\begin{Schunk}
\begin{Sinput}
 #j sample of 10 cars
 nsim = 5000
 pi.coverage=rep(NA, nsim)
 for(i in 1:nsim){
   tmp.fit=lm(mpg~wt, data=mtcars, subset=sample(dim(mtcars)[1], 10))
   preds = predict(tmp.fit, newdata= data.frame(wt=x), interval="prediction")
   pi.coverage[i] = 1-sum(y>preds[,3] | y<preds[,2])/npred
 }
 sum(pi.coverage)/nsim
\end{Sinput}
\end{Schunk}
In the above code, the $\tilde{y}$ were generated using a Gaussian distribution and this gave us a bit of an unfair advantage.  We could also generate $\tilde{y}$ by sampling from the residuals.  This is fairer since the data are not exactly normal. However, the average coverage is close to 95\% even with new data generated by sampling from the residuals.
\begin{Schunk}
\begin{Sinput}
 y = alpha+beta*x+sample(residuals(fit), npred, replace=TRUE)
 nsim = 5000
 pi.coverage=rep(NA, nsim)
 for(i in 1:nsim){
   tmp.fit=lm(mpg~wt, data=mtcars, subset=sample(dim(mtcars)[1], 10))
   preds = predict(tmp.fit, newdata= data.frame(wt=x), interval="prediction")
   pi.coverage[i] = 1-sum(y>preds[,3] | y<preds[,2])/npred
 }
 sum(pi.coverage)/nsim
\end{Sinput}
\end{Schunk}

\subsection{Computing prediction intervals via bootstrapping}
In the same way that we generated confidence intervals via bootstrapping, we also can generate prediction intervals from bootstrapping. The idea is fairly simple.  Say we want to generate a prediction interval for weight $x$.  We fit a model to the data and then use the fitted model to generate bootstrap data. We fit to that bootstrap data to get bootstrap parameter estimates including an estimate of the residual variance. We then generate a new data point for $x$ using those parameter estimates.  We repeat this thousands of times to get many predictions for $x$ and the 95\% quantiles are the bootstapped prediction intervals at $x$.  



\subsection{Prediction interval includes two types of uncertainty}

Includes uncertainty about the relationship between mpg and weight, i.e. in the regression parameters, and uncertainty in what $\tilde{y}$ will be due to observation error.

We will not get the right prediction interval if we simply use our estimated regression line and add on the estimated residual variance.  I think this might work though: bootstrapping a bunch of regression lines and simulating new data based on $\hat{\alpha}_b + \hat{\beta}_b x + e$ with $e\sim \N(0,\hat{\sigma^2}_b)$.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]
\begin{center}
\end{center}
\caption{Comparison of four ways to compute prediction intervals.  There is no one true CI as there are many CI algorithms that would could produce proper coverage; i.e. that the $x$-CI would cover the true relationship $x$ percent of the time.  The red line CIs are based on the true distribution of the $\alpha$ and $\beta$ from all the 10 car samples from the original 32 car dataset.  A CI based on this true distribution is simple one of the CIs that will have proper coverage.}
\label{fig:PIs}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


\subsection{CI computation for the parameters of MARSS models}

The MARSS package will allow you to construct CIs via an estimated Hessian, parametric bootstrapping, or residuals bootstrapping.   Construction of CIs using an estimated Hessian distribution is the fastest and it the default approach for producing CIs for the parameter estimates  via function \verb@MARSSparamCIs()@.   The residuals bootstrap, called innovations bootstrapping for MARSS models, is useful if one is unwilling to assume a particular distribution for the errors. Residuals bootstrapping is selected by passing in \verb@method="innovations"@ to  \verb@MARSSparamCIs()@.  The parametric bootstrap is useful when the estimate of the Hessian is numerically difficult or the multivariate normality assumption for the parameters is dubious. Parametric bootstrapping is selected by passing in \verb@method="parametric"@ to  \verb@MARSSparamCIs()@. It should be kept in mind that the methods discussed here do not work that well if the likelihood surface is multi-modal.

If one needs CIs for a metric that is some function of the estimated parameters, then CIs can be constructed using the intervals (e.g. 95\%) from a large number of bootstrapped parameter estimates.  The function \verb@MARSSboot@ will generate bootstrap parameter estimates via an estimated Hessian, residuals bootstrapping or parametric bootstrapping.


\section{Expected values of states and data in MARSS models}

Unlike a linear regression, a MARSS model has two types of random variables: the new data ($\y\tilde{y}$) like a regression but also the state ($\xx$).  In a linear regression, we are uncertain about the relationship (the red line i Figure \ref{fig:CI.basic}) because we are uncertain about the parameters that describe that line.  In a MARSS model, we have the uncertainty about the parameters, but even if we did not, we would still be uncertain about the $\xx$ that the $\y\tilde{y}$ are observations of because $\xx$ is a random process.

%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
\xx_t = \xx_{t-1}+\ww_t \text{ where } \ww_t \sim \MVN(0,\QQ) \\
\yy_t = \ZZ\xx_t+\aa+\vv_t \text{ where } \vv_t \sim \MVN(0,\RR)  \\
\xx_0 \sim \MVN(\pipi,\LAM) 
\end{gathered}   
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~

In a linear regression, you, typically, plot the regression line on to the data (as in Figure \ref{fig:CI.basic}a).  The regression line is the expected value of new $y$ at a given $x$ given the estimated parameter values.  You can get the expected value of $\tilde{y}$ from a linear regression fit using either the \verb@fitted@ or \verb@predict@ function.

In a MARSS model, the expected value of $\y\tilde{y}$ has the same interpretation but the calculation of the expected value involves both the estimated parameters and the expected value of $\xx$:
\begin{equation}
\E[\y\tilde{y}_t] = \hat{\ZZ}\E[\xx_t] + \hat{\aa}
\end{equation}
where the expectation is conditioned on the data ($\yy$).  You can get the expected value of $\y\tilde{y}$ from a MARSS fit using either the \verb@fitted@ or \verb@predict@ function:
\begin{Schunk}
\begin{Sinput}
 #show example
\end{Sinput}
\end{Schunk}

Often it is the case that the objective of the analysis is to estimate $\xx$ and it is its expected value that is desired.  

Show standard error of $\xx$.  That makes sense.  We don't know what $\xx$ is.

We can also the show the stand. error for functions of $\xx$.  We could show the standard error of $\hat{\ZZ}\E[\xx_t] + \hat{\aa}$ .  That however is NOT the variability of $\E[\y\tilde{y}]$.  If we just collected a bunch of new data for the same time period, the $\xx$ stays the same.  The s.e. reflects our uncertainty but the $\xx$ is not changing.  $\E[\y\tilde{y}]=\ZZ\xx+\uu$ and uncertainty about that is due to our uncertainty in both $\xx$ and the parameters.  We are uncertain about $\xx$ in the same way as we are uncertain about the red line.  Confidence interval not standard error.  The standard error of $\yy$ is from $\RR$.  E(param estimate) have standard errors and $\xx_t$ has a standard error.

Doesn't make sense to use that to compute the standard error of $\yy$ unless we wanted to show the stand. error if the whole process were run again or run forward.  We assume the parameters are at their estimated values and run forward. But why would the new process be governed by the estimated parameters?

Expected value of $\yy$ if we ran the process over and over and each time generated a new $\xx$ using the estimated parameters.  ??? 
\xx_t = \hat{\BB}\xx_{t-1}+\hat{\uu+\ww_t \text{ where } \ww_t \sim \MVN(0,\hat{\QQ}) \\

Forecasting using the estimated values.  Yes, standard thing to do but keep in mind that it the prediction intervals will be too narrow.


why show variability in the E(y)? You are uncertain about the parameters.  s.e. of the states has nothing to do with the uncertainty in the parameters.  You are uncertain about the states even if you are certain about the 

\section{Confidence intervals and prediction intervals for MARSS models}


%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\setkeys{Gin}{}
\begin{figure}[htp]
\begin{center}
\end{center}
\caption{Distribution of $\alpha$ and $\beta$ estimates from parametrically bootstrapping using the fit to $\yy_j$.  You can see that they are approximately multivariate normal even for $n=10$ (so not $n$ large).  Note, these estimates are from lm() which is using least-squares estimation, but the parameter estimates are the same as the maximum-likelihood estiamates for this problem (linear regression with Gaussian errors).}
\label{fig:dist.of.ests}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\section{Functions used in the chapter}\label{sec:funcs}

\subsection{Analytical CIs}

This function computes the analytical CIs with the Fisher information matrix using the known (true) $\sigma$ if passed in or estimated $\sigma$ if not.
\begin{Schunk}
\begin{Sinput}
 CI.analytical.true.s2=function(dat, x, sigma, alp=0.05){
   fit=lm(y~x, data=dat)
   Xi = cbind(1, dat$x)
   XiXi.inv = solve(t(Xi)%*%Xi)
   theta = matrix(coef(fit), ncol=1)
   if(is.null(sigma)){
     sigma = sqrt(sum(fit$residual^2)/fit.df)
   }
   Sigma = sigma^2*XiXi.inv
   X = cbind(1, x)
   EyX = X%*%theta
   #the analytical CI
   CI = EyX + qnorm(c(alp/2,1-alp/2)) * sqrt(X%*%Sigma%*%t(X))
   c(fit=EyX, lwr=CI[1], upr=CI[2])
 }
\end{Sinput}
\end{Schunk}

This function computes the analytical CIs with the observed Fisher information matrix using the estimated $\sigma$. This is biased.
\begin{Schunk}
\begin{Sinput}
 CI.analytical.est.s2=function(dat, x, alp=0.05){
   fit=lm(y~x, data=dat)
   Xi = cbind(1, dat$x)
   XiXi.inv = solve(t(Xi)%*%Xi)
   theta = matrix(coef(fit), ncol=1)
   fit.df=fit$df.residual
   sigma = sqrt(sum(fit$residual^2)/fit.df)
   Sigma = sigma^2*XiXi.inv
   X = cbind(1, x)
   EyX = X%*%theta
   #the analytical CI
   CI = EyX + qnorm(c(alp/2,1-alp/2)) * sqrt(X%*%Sigma%*%t(X))
   c(fit=EyX, lwr=CI[1], upr=CI[2])
 }
\end{Sinput}
\end{Schunk}

This function computes analytical CIs with the observed Fisher information matrix and corrects using the quantiles for the t-distribution.
\begin{Schunk}
\begin{Sinput}
 CI.analytical.corrected=function(dat, x, alp=0.05){
   fit=lm(y~x, data=dat)
   fit.df = fit$df.residual
   Xi = cbind(1, dat$x)
   XiXi.inv = solve(t(Xi)%*%Xi)
   hat.sigma = sqrt(sum(fit$residual^2)/fit.df)
   hat.Sigma = hat.sigma^2*XiXi.inv
   hat.theta = matrix(coef(fit), ncol=1)
   X = cbind(1, x)
   EyX = X%*%hat.theta
   CI = EyX + qt(c(alp/2,1-alp/2), df=fit.df) * sqrt(X%*%hat.Sigma%*%t(X))
   c(fit=EyX, lwr=CI[1], upr=CI[2])
 }
\end{Sinput}
\end{Schunk}

\subsection{Numerical estimation of the Hessian of the negative log-likelihood function}

The following function numerically estimates the Hessian of the negative of the log-likelihood functions and inverts that to give us an estimate of $\Sigma$:
\begin{Schunk}
\begin{Sinput}
 #Get the MLEs and MLE Sigma
 hessian.parm=function(dat){
   library(MASS)
   NLL <- function(parm, y=NULL, x=NULL) {
     resids = y - x * parm[2] - parm[1]
     dresids = suppressWarnings(dnorm(resids, 0, parm[3], log = TRUE))
     -sum(dresids)
   }
   fit=lm(y~x, data=dat)
   sigma = sqrt(sum(fit$residual^2)/fit$df.residual)
   alpha=coef(fit)[1]
   beta=coef(fit)[2]
   pars=c(alpha, beta, sigma)
   names(pars)=c("alpha","beta","sigma")
   
   fit.tmp=optim(pars, NLL, y=dat$y, x=dat$x, hessian=TRUE)
   parSigma = solve(fit.tmp$hessian)
   parMean = matrix(fit.tmp$par, ncol=1)
   names(parMean)=c("alpha","beta","sigma")
   
   list(parMean=parMean, parSigma=parSigma)
 }
\end{Sinput}
\end{Schunk}

\subsection{Functions for producing bootstrapped parameter estimates}

This function produces parameter estimates by drawing from the estimated $\Sigma$:
\begin{Schunk}
\begin{Sinput}
 hessian.boot=function(dat, nboot=1000){
   hes=hessian.parm(dat)  
   #generate alpah and beta from Sigma
   boot.params = mvrnorm(nboot, mu = hes$parMean, Sigma = hes$parSigma)
   colnames(boot.params)=c("alpha","beta","sigma")
   boot.params
 }
\end{Sinput}
\end{Schunk}

This function produces parameter estimates by parametric bootstrapping:
\begin{Schunk}
\begin{Sinput}
 parametric.boot=function(dat, nboot=1000){
   #first fit model to data
   fit=lm(y~x, data=dat)
   #x's at which to generate new data
   x=dat$x
   #matrix to store the estimates
   boot.params=matrix(NA,nboot,3)
   sigma = sqrt(sum(fit$residual^2)/fit$df.residual)
   alpha=coef(fit)[1]
   beta=coef(fit)[2]
   for(i in 1:nboot){
     y=alpha + beta*x + rnorm(nrow(dat),0,sigma)
     tmp.fit=lm(y~x)
     boot.params[i,]=c(coef(tmp.fit), 
                       sqrt(sum(tmp.fit$residual^2)/tmp.fit$df.residual))
   }
   colnames(boot.params)=c("alpha","beta","sigma")
   boot.params
 }
 parametric.boot.cis=function(dat, x, nboot=1000){
   boot.params=parametric.boot(dat, nboot=nboot)
   boot.CI(boot.params, x)
 }
\end{Sinput}
\end{Schunk}

This function produces parameter estimates by resampling from the residuals:
\begin{Schunk}
\begin{Sinput}
 residuals.boot=function(dat, nboot=1000){
   fit=lm(y~x, data=dat)
   resids=residuals(fit)  
   alpha=coef(fit)[1]
   beta=coef(fit)[2]
   boot.params=matrix(NA,nboot,3)
   n = nrow(dat) #number of data points
   for(i in 1:nboot){
     tmp = sample(n, replace=TRUE)
     tmp.y=alpha + beta*dat$x + resids[tmp]
     tmp.fit=lm(tmp.y~dat$x)
     boot.params[i,]=c(coef(tmp.fit), 
                       sqrt(sum(tmp.fit$residual^2)/tmp.fit$df.residual))
   }
   colnames(boot.params)=c("alpha","beta","sigma")
   boot.params
 }
\end{Sinput}
\end{Schunk}

This function produces parameter estimates by resampling the data:
\begin{Schunk}
\begin{Sinput}
 resampling.boot=function(dat, nboot=1000){
   boot.params=matrix(NA,nboot,3)
   n = nrow(dat) #number of data points
   for(i in 1:nboot){
     #sample with replacement
     tmp = sample(n, replace=TRUE)
     #tmp.fit is the fit to this bootstrapped data
     tmp.fit=lm(y~x, data=dat, subset=tmp)
     boot.params[i,]=c(
       coef(tmp.fit),
       sqrt(sum(tmp.fit$residuals^2)/tmp.fit$df.residual))
   }
   colnames(boot.params)=c("alpha","beta","sigma")
   boot.params
 }
\end{Sinput}
\end{Schunk}

\subsection{Functions for producing CIs from bootstrapped parameter estimates}

This function produces CIs at $x$ from a set of bootstrapped parameter estimates:
\begin{Schunk}
\begin{Sinput}
 boot.CI=function(boot.params, x, alp=0.05){
   CIs=apply(
     boot.params[,c("alpha","beta"),drop=FALSE]%*%rbind(1,x),
     2,quantile, c(0.5, alp/2, 1-alp/2) )
   colnames(CIs)=x
   t(CIs) #to look like predict output
 }
\end{Sinput}
\end{Schunk}

This function is then combined with the functions for producing bootstrapped parameter estimates to produces CIs for the different bootstrap methods:
\begin{Schunk}
\begin{Sinput}
 hessian.boot.cis=function(dat, x, nboot=1000){
   boot.params=hessian.boot(dat, nboot=nboot)
   boot.CI(boot.params, x)
 }
 parametric.boot.cis=function(dat, x, nboot=1000){
   boot.params=parametric.boot(dat, nboot=nboot)
   boot.CI(boot.params, x)
 }
 residuals.boot.cis=function(dat, x, nboot=1000){
   boot.params=residuals.boot(dat, nboot=nboot)
   boot.CI(boot.params, x)
 }
 resampling.boot.cis=function(dat, x, nboot=5000){
   boot.params=resampling.boot(dat, nboot=nboot)
   boot.CI(boot.params, x)
 }
\end{Sinput}
\end{Schunk}

\subsection{Estimating the correction for a CI via bootstrapping}

\begin{Schunk}
\begin{Sinput}
 ci.adj
\end{Sinput}
\begin{Soutput}
function(dat, x, nboot1=5000, nboot2=1000, type="hessian"){
  #x is the x where the ci is computed; one value
  #type is analytical, hessian, parametric or residuals
  nsamp=nrow(dat)
  fit=lm(y~x, data=dat)
  sigma = sqrt(sum(fit$residuals^2)/fit$df.residual)
  alpha=coef(fit)[1]
  beta=coef(fit)[2]
  #this is the correct width of the 95\% CI
  #for a model with the parameters estimated from dat
  ci=parametric.boot.cis(dat,x,nboot=nboot1)[2:3]
  ci.width=ci[2]-ci[1]
  #ci.width.boot will hold a set of bootstrapped CIs
  ci.width.boot=rep(NA,nboot2)
  for(i in 1:nboot2){
    if(type %in% c("parametric", "hessian", "analytical")){
      y=alpha + beta*dat$x + rnorm(nsamp,0,sigma)
      tmp.dat=data.frame(x=dat$x, y=y)
      if(type=="analytical") tmp.ci=CI.analytical.est.s2(tmp.dat,x)[2:3]
      if(type=="parametric") tmp.ci=parametric.boot.cis(tmp.dat,x,nboot1)[2:3]
      if(type=="hessian") tmp.ci=hessian.boot.cis(tmp.dat,x,nboot1)[2:3]
    }
    if(type == "residuals"){
      y=alpha + beta*dat$x + sample(fit$residuals, nsamp)
      tmp.ci=residuals.boot.cis(tmp.dat,x,nboot1)[2:3]
    }
    if(type == "resampling"){
      tmp.dat = dat[sample(nsamp, replace=TRUE),]
      tmp.ci=resampling.boot.cis(tmp.dat,x,nboot1)[2:3]
    }
    ci.width.boot[i]=tmp.ci[2]-tmp.ci[1]
  }
  #Trim to deal with random ouliers
  mean(ci.width/ci.width.boot, trim=.1)
}
\end{Soutput}
\end{Schunk}
