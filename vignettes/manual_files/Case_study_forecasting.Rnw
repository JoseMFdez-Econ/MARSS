\SweaveOpts{keep.source=TRUE, prefix.string=./figures/Forecast-, eps=FALSE, split=TRUE}
<<RUNFIRST, echo=FALSE>>=
require(MARSS)
options(prompt=" ", continue=" ", width=60)
@
\chapter{Forecasting with MARSS models}
\label{chap:Forecast}
\chaptermark{Forecasting}
%Add footnote with instructions for getting code
\blfootnote{Type \texttt{RShowDoc("Chapter\_Forecast.R",package="MARSS")} at the R command line to open a file with all the code for the examples in this chapter.}

In this chapter, we illustrate some ways to build forecast models using MARSS models.

\section{Local level model with no explanatory variables}

We will start with a simple example from \citep{Petrisetal2009} where we forecast the Nile River flow.  The Nile River flow data is in the datasets package:
<<Cs01, eval=TRUE, echo=TRUE>>=
#load the datasets package
library(datasets)
data(Nile)   #load the data
#The data is in a ts format, and we need a matrix
#year is in the first row
dat = rbind(year=time(Nile),flow=Nile)
@

We will use what is often termed a ``local level model''.  In this case, the observed flow is modelled as an observation of the average river flow at year $t$ (level or $x_t$) and the level is modelled as a random walk:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
x_t = x_{t-1}+w_t \text{ where } w_t \sim \N(0,q) \\
y_t = x_t+v_t \text{ where } v_t \sim \N(0,r)  \\
x_0 = \pi 
\end{gathered}   
\label{eq:forecast.nile.1}\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~
$y_t$ is the river flow volume measured at year $t$ and $x_t$ is the mean level. The model is specified as:

<<Cs02, eval=TRUE>>=
mod.nile = list(
Z=matrix(1), A=matrix(0), R=matrix("r"),
B=matrix(1), U=matrix(0), Q=matrix("q"),
x0=matrix("pi") )
@

We fit the model as follows.
<<Cs03, eval=TRUE>>=
fit.nile = MARSS(dat["flow",], model=mod.nile)
@

For a one-step ahead forecast, we want the expected value of $y_t$ conditioned on $y_1$ to $y_{t-1}$ and the maximum-likelihood parameter estimates: $\E(y_t|\Theta_{MLE}, y_{)1:t-1}$.  This is:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
\E(y_t|\Theta_{MLE}, y_{)1:t-1} = \ZZ_{MLE}\E(x_t|\Theta_{MLE}, y_{)1:t-1} + \aa_{MLE}
\end{gathered}   
\label{eq:exp.y.nile}\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~

$\E(x_t|\Theta_{MLE}, y_{)1:t-1}$ is standard output from the Kalman filter and in the case of model \ref{eq:forecast.nile.1}, $\ZZ=1$ and $\aa=0$ so $\E(y_t|\Theta_{MLE}, y_{)1:t-1} = \E(x_t|\Theta_{MLE}, y_{)1:t-1}$.  Thus we could just use the output from the Kalman filter, however this will not be the case if $\ZZ$ is not identity or $\aa$ is zero, nor would it be the case if there were inputs ($\CC\cc_t$) in the $\yy_t$ equation.  You could compute $\E(y_t|\Theta_{MLE}, y_{)1:t-1}$ using equation \ref{eq:exp.y.nile} in that case, but the MARSS package has a function to do this for you: \verb@MARSShatyt@.

The one-step ahead forecast of the Nile River flow at time $t$ is:
<<Cs05>>=
forecast_nile = MARSShatyt(fit.nile)$ytt1
@

Figure \ref{{fig:CS6.fig1} shows the one-step ahead predictions using the MLE model with the actual data.  We might see what would happen if we increased the signal ($q$) to noise ($r$) ratio.  Let's set it to one so we set $r$ equal to the MLE value and then set $q$ equal to the MLE $r$:
<<Cs06, eval=TRUE>>=
mod.nile2 = list(
Z=matrix(1), A=matrix(0), R=coef(fit.nile)$R,
B=matrix(1), U=matrix(0), Q=coef(fit.nile)$R,
x0=matrix("pi") )
@

We fit the model as follows.
<<Cs07, eval=TRUE>>=
fit.nile2 = MARSS(dat["flow",], model=mod.nile2, silent=TRUE)
forecast_nile2 = MARSShatyt(fit.nile2)$ytt1
@

The result is shown in the blue line in Figure \ref{{fig:CS6.fig1}.  It appears that we follow the data better but that is just an illusion.  The actual mean squared error is larger.  This is a property of the MLE parameters; they minimize the mean squared one-step ahead error.

\begin{figure}[htp]
\begin{center}
<<Cs05, eval=TRUE, echo=FALSE, fig=TRUE>>=
plot(dat["year",],dat["flow",],ylab="Flow volume",xlab="", type="l", lwd=2)
lines(dat["year",],forecast_nile[1,], col="red", lwd=2)
lines(dat["year",],forecast_nile2[1,], col="blue", lwd=1)
title("Nile River flow with one step ahead prediction")
mserr = round(mean((dat["flow",]-forecast_nile[1,])^2), digits=0)
mserr2 = round(mean((dat["flow",]-forecast_nile2[1,])^2), digits=0)
legend("topright", paste("mean sq err =",c(mserr,mserr2)), bty="n", col=c("red","blue"),lty=1)
@
\end{center}
\caption{The Nile River flow volume 1871 to 1970 (black line). The predicted flow at year $t$ given the data up to year $t-1$ with the MLE parameters (red line) and with the signal to noise ratio equal to 1 ($q=r_{MLE}$).}
\label{fig:CS6.fig1}
\end{figure}

If you are familiar with the \verb@StructTS@ function, the equivalent model is fit as:
<<Cs03, eval=TRUE>>=
fit.structTS = StructTS(Nile, type = "level")
@
You can get a one-step ahead prediction using 
<<Cs03, eval=TRUE, echo=FALSE>>=
predict(fit.structTS)
@
but this will just be for 1971, one year after the end of the Nile time series.  \verb@KalmanRun@ will give you $\E(x_t|\Theta_{MLE},y_{1:t}$ and \verb@KalmanSmooth@ will give you $\E(x_t|\Theta_{MLE},y_{1:T})$. Neither return $\E(x_t|\Theta_{MLE},y_{1:t-1})$, $\E(y_t|\Theta_{MLE},y_{1:t})$, $\E(y_t|\Theta_{MLE},y_{1:T})$ or $\E(y_t|\Theta_{MLE},y_{1:t-1})$.  These are returned respectively in MARSS with \verb@MARSSkf(fit)$xtt@, \verb@MARSSkf(fit)$xtT@, \verb@MARSSkfss(fit)$xtt1@, \verb@MARSShatyt(fit)$ytt@, \verb@MARSShatyt(fit)$ytt1@, and \verb@MARSShatyt(fit)$ytT@.


\section{Local level model with explanatory variables}
Now we want to add some explanatory variables for the fluctuations in level ($w_t$).
The local level model with explanatory variables for $x_t$ is
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
x_t = x_{t-1}+\CC\cc_t+w_t \text{ where } w_t \sim \N(0,q) \\
y_t = x_t+v_t \text{ where } v_t \sim \N(0,r)  \\
x_0 = \pi 
\end{gathered}   
\label{eq:forecast.nile.exp.var}
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~
$\cc_t$ are the explanatory variables at time $t$ and missing values are allowed and $\CC$ is the effect of $\cc_t$.  Setting the model up like this means you have to be careful about collinearity in $\cc$ otherwise you create ridges in the likelihood surface associated with $\CC$ and that causes well-known problems with estimation and unidentifiability, just like in regular multivariate regression.

\subsection{An impulse explanatory variable}

A dam was constructed on the Nile in 1898, so a logical explanatory variable is `dam construction'.  This will be a 0 in all years except 1898.  This will allow the level, $x_t$, to change abrubtly in 1898.  We create this indicator variable like so:
<<Cs02, eval=TRUE>>=
dam = matrix(0,1,length(Nile))
dam[time(Nile)==1898]=1
@
It is all zero except in 1898.  MARSS wants things specified as matrices with time across the columns.

<<Cs02, eval=TRUE>>=
mod.nile3 = list(
Z=matrix(1), A=matrix(0), R=matrix("r"),
B=matrix(1), U=matrix(0), Q=matrix("q"),
x0=matrix("pi"),
C=matrix("dam"),c=dam)
@

<<Cs02, eval=TRUE>>=
fit.nile3 = MARSS(dat["flow",], model=mod.nile3)
forecast_nile3 = MARSShatyt(fit.nile3)$ytt1
@

Notice it did not converge.  The state (level) variance is going to zero. The fit (Figure \ref{fig:fore.nile3}) looks like a pre- and post- flat levels.  Why did the fit do that?  Well, look at the flow after 1900.  That does not look like a non-stationary random walk at all, which is the model we are using for the level.  It looks like a process that fluctuates about some fixed level---exactly as the fit shows.  Why did it only do this when we included the dam indicator variable?  Before the model needed $q>0$ to fit the shift in 1898-1899.  Now we added a parameter to explain that and $q$ can now go to zero.  Notice that although the fit looks visually worse, it's actually better.  The mean squared error went down.

\begin{figure}[htp]
\begin{center}
<<Cs05, eval=TRUE, echo=FALSE, fig=TRUE>>=
plot(dat["year",],dat["flow",],ylab="Flow volume",xlab="", type="l", lwd=2)
lines(dat["year",],forecast_nile3[1,], col="red", lwd=2)
mserr = round(mean((dat["flow",]-forecast_nile3[1,])^2), digits=0)
legend("topright", paste("mean sq err =",mserr), bty="n", col=c("red"),lty=1)
title("Nile River one step ahead prediction with dam effect")
@
\end{center}
\caption{The Nile River flow volume 1871 to 1970 (black line). The predicted flow at year $t$ given the data up to year $t-1$ using a model that allows a shift in the mean level in 1898-1899.}
\label{fig:fore.nile3}
\end{figure}

The supplementary examples at the end of this chapter shows you how to some alternative models that have a flat level that one might use to try to model the fluctuations around the flat level.

There are two parameters, σ^2_ξ and σ^2_eps. It is an ARIMA(0,1,1) model, but with restrictions on the parameter set.

The local linear trend model, type = "trend", has the same measurement equation, but with a time-varying slope in the dynamics for m[t], given by

m[t+1] = m[t] + n[t] + xi[t], xi[t] ~ N(0, σ^2_ξ)

n[t+1] = n[t] + ζ[t], ζ[t] ~ N(0, σ^2_ζ)

with three variance parameters. It is not uncommon to find σ^2_ζ = 0 (which reduces to the local level model) or σ^2_ξ = 0, which ensures a smooth trend. This is a restricted ARIMA(0,2,2) model.

The basic structural model, type = "BSM", is a local trend model with an additional seasonal component. Thus the measurement equation is

x[t] = m[t] + s[t] + eps[t], eps[t] ~ N(0, σ^2_eps)

where s[t] is a seasonal component with dynamics

s[t+1] = -s[t] - … - s[t - s + 2] + w[t], w[t] ~ N(0, σ^2_w)

The boundary case σ^2_w = 0 corresponds to a deterministic (but arbitrary) seasonal pattern. (This is sometimes known as the ‘dummy variable’ version of the BSM.)

\begin{figure}[htp]
\begin{center}
<<Cs_ukgas, eval=TRUE, echo=TRUE, fig=TRUE>>=
library(datasets)
plot(log10(UKgas),col="blue",type="p")
fit <- StructTS(log10(UKgas), type = "level")
lines(fitted(fit),col="red")
fit <- StructTS(log10(UKgas), type = "trend")
lines(c(time(UKgas)),apply(fitted(fit),1,sum),col="green")
fit <- StructTS(log10(UKgas), type = "BSM")
lines(c(time(UKgas)),apply(fitted(fit),1,sum),col="black")
legend("topright", paste("mean sq err =",c(mserr,mserr2)), bty="n", col=c("red","blue"),lty=1)
@
\end{center}
\caption{The UK gas dataset and fit from StructTS (stats package) with different types of structural time-series models: level, level+trend, level+trend+season.}
\label{fig:CS.ukgas}
\end{figure}



\section{Supplementary examples}

This shows some ways to model flat levels with fluctuations.  In each case, the mean level pre- and post-dam is modeled with $\DD$.  Essentially, what we are doing is removing the mean level and then modelling the fluctuations about the demeaned flow.

<<load.nile.data.again>>=
data(Nile)   #load the data
#The data is in a ts format, and we need a matrix
#year is in the first row
dat = rbind(year=time(Nile),flow=Nile)
@

A flat pre- and post- level model.  The levels are in $\DD$ and there is error around that.  $\xx$ is fixed at zero since it does not appear in the model. We pass in inits for $\DD$ to speed things up.
<<nile.flat.level, results=hide>>=
dam = matrix(0,2,length(Nile))
dam[1,time(Nile)<=1897]=1
dam[2,time(Nile)>1897]=1

mod.nile.s1 = list(
  Z=matrix(1), A=matrix(0), R=matrix("r"),
  B=matrix(1), U=matrix(0), Q=matrix(0),
  x0=matrix(0),
  D=matrix(c("pre","post"),2,1),d=dam)

fit.nile.s1 = MARSS(dat["flow",], model=mod.nile.s1,, 
                  inits=list(D=matrix(c(1100,850),2,1)))
forecast.nile.s1 = MARSShatyt(fit.nile.s1)$ytt1
@

A flat pre- and post-level model but now we have AR(1) stationary error added.  By setting $\uu=0$ we force the mean of the AR(1) to be zero.  
<<nile.flat.level.ar1, results=hide>>=
mod.nile.s2 = list(
  Z=matrix(1), A=matrix(0), R=matrix("r"),
  B=matrix("b"), U="zero", Q=matrix("q"),
  x0=matrix(0),
  D=matrix(c("pre","post"),1,2),d=dam)

fit.nile.s2 = MARSS(dat["flow",], model=mod.nile.s2, 
                  inits=list(D=matrix(c(1100,850),2,1)))
forecast.nile.s2 = MARSShatyt(fit.nile.s2)$ytt1
@

A flat pre- and post-level model but now we have MA(1) stationary error added.  Notice how $\xx$ is now $2 \times 1$ and $\BB$ is $2 \times 2$. The form of the $\xx$ part of the equation is how one creates and $\xx$ that represents an error term that is MA(1).  This is a bit different than a standard MA(1) since we have added non-MA(1) error (in $r$).
<<nile.flat.level.ma1, results=hide>>=
mod.nile.s3 = list(
  Z=matrix(list(1,"phi"),1,2), A=matrix(0), R=matrix("r"),
  B=matrix(c(0,1,0,0),2,2), U="zero", 
  Q=matrix(list("q",0,0,0),2,2),
  x0="zero",
  D=matrix(c("pre","post"),1,2),d=dam)

fit.nile.s3 = MARSS(dat["flow",], model=mod.nile.s3, 
                  inits=list(D=matrix(c(1100,850),2,1)))
forecast.nile.s3 = MARSShatyt(fit.nile.s3)$ytt1
@
If you want a standard MA(1) error, then set $r=0$ and use \verb@method="BFGS"@ in the \verb@MARSS()@ call.

There is nothing gained by these more elaborate models of the error however.  Here are the mean squared errors of the one-step ahead forecasts are compared.
<<comp.msee>>=
c(
S1=round(mean((dat["flow",]-forecast.nile.s1[1,])^2), digits=0),
S2=round(mean((dat["flow",]-forecast.nile.s2[1,])^2), digits=0),
S3=round(mean((dat["flow",]-forecast.nile.s3[1,])^2), digits=0))
@
and here are the AICc:
<<comp.nile.aicc>>=
c(
S1=fit.nile.s1$AICc,
S2=fit.nile.s2$AICc,
S3=fit.nile.s3$AICc)
@

\section{Relationship of the random walk observed with error to ARIMA(0,1,1)}
Here we show how a the local level model is equivalent to a ARIMA(0,1,1), or equivalently that the first difference of the $y_t$ in a local level model.  The local level model or random walk observed with error is
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
\begin{gathered}
x_t = x_{t-1}+w_t \text{ where } w_t \sim \N(0,q) \\
y_t = x_t+v_t \text{ where } v_t \sim \N(0,r)  \\
x_0 = \pi 
\end{gathered}   
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~

Thus 
\begin{equation}
\begin{gathered}
m_t \equiv y_t - y_{t-1} = x_t - x_{t-1} + v_t - v_{t-1} \\
m_t \equiv y_t - y_{t-1} = w_t + v_t - v_{t-1}
\end{gathered}   
\label{eqn:rw.as.ma1}
\end{equation}

A univariate MA(1) is $m_t = e_t + \theta e_{t-1}$ where $e \sim \N(0, \sigma^2)$. The key statistical properties of a MA(1) are $m_t$ and $m_{t-1}$ are correlated and $m_t$ and $m_{t+h}$, $h>1$ are uncorrelated.  The variance of $m_t$ is  $(1+\theta^2) \sigma^2$ and the covariance of $m_t$ and $m_{t-1}$ is $\theta \sigma^2$.

Thus $v_t - v_{t-1}$ is a MA(1) with $\theta=1$ and $\sigma^2 = r$.  $w_t + v_t - v_{t-1}$ in Equation \ref{eqn:rw.as.ma1} is still MA(1) as we have reduced but not eliminated the lag-1 correlation and not added any lag-2 correlation.

$w_t + v_t - v_{t-1}$ has a variance of $q + 2r$, which you see by noting that $w_t$, $v_t$ and $v_{t-1}$ are all independent by definition.  The covariance of $m_t$ and $m_{t-1}$ is  $-r$ since
\begin{equation}
\begin{gathered}
m_t &\equiv w_t + v_t - v_{t-1}\\
\mean(m_t) &= 0\\
\cov(m_t,m_{t-1}) &= \E((w_t + v_t - v_{t-1})(w_{t-1} + v_{t-1} - v_{t-2}))\\
&= \E(- v_{t-1} v_{t-1}) \textrm{since all the other expectations are 0\\
&= - r
\end{gathered}   
\end{equation}

We need to find the $\theta$ and $\sigma^2$ values that produce a MA(1) with the above variance and covariance properties.
\begin{equation}
\begin{gathered}
q+2r = (1+\theta^2) \sigma^2\\
-r = \theta \sigma^2\\

-r/(q+2r) = \theta/(1+\theta^2)\\
\theta^2 + \theta (q+2r)/r + 1 = 0\\
\textrm{, }\theta \textrm{ must be negative}\\

sigma^2 = -r/theta
\end{gathered}   
\end{equation}

<<reset, echo=FALSE, include.source=FALSE>>=
options(prompt="> ", continue=" +", width=120)
@