%\VignetteIndexEntry{Residuals} 
%\VignettePackage{MARSS}
\documentclass[]{article}
%set margins to 1in without fullsty
	\addtolength{\oddsidemargin}{-.875in}
	\addtolength{\evensidemargin}{-.875in}
	\addtolength{\textwidth}{1.75in}

	\addtolength{\topmargin}{-.875in}
	\addtolength{\textheight}{1.75in}
%\usepackage{fullpage} %more standardized margins

% choose options for [] as required from the list
% in the Reference Guide, Sect. 2.2

\usepackage{multirow}
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage[round]{natbib}

% Math stuff
\usepackage{amsmath} % the standard math package
\usepackage{amsfonts} % the standard math package
%%%% bold maths symbol system:
\def\uupsilon{\pmb{\upsilon}}
\def\llambda{\pmb{\lambda}}
\def\bbeta{\pmb{\beta}}
\def\aalpha{\pmb{\alpha}}
\def\zzeta{\pmb{\zeta}}
\def\etaeta{\mbox{\boldmath $\eta$}}
\def\xixi{\mbox{\boldmath $\xi$}}
\def\ep{\mbox{\boldmath $\epsilon$}}
\def\DEL{\mbox{\boldmath $\Delta$}}
\def\PHI{\mbox{\boldmath $\Phi$}}
\def\PI{\mbox{\boldmath $\Pi$}}
\def\LAM{\mbox{\boldmath $\Lambda$}}
\def\LAMm{\mathbb{L}}
\def\GAM{\mbox{\boldmath $\Gamma$}}
\def\OMG{\mbox{\boldmath $\Omega$}}
\def\SI{\mbox{\boldmath $\Sigma$}}
\def\TH{\mbox{\boldmath $\Theta$}}
\def\UPS{\mbox{\boldmath $\Upsilon$}}
\def\XI{\mbox{\boldmath $\Xi$}}
\def\AA{\mbox{$\mathbf A$}}	\def\aa{\mbox{$\mathbf a$}}
\def\Ab{\mbox{$\mathbf D$}} \def\Aa{\mbox{$\mathbf d$}} \def\Am{\PI}
\def\BB{\mbox{$\mathbf B$}}	\def\bb{\mbox{$\mathbf b$}} \def\Bb{\mbox{$\mathbf J$}} \def\Ba{\mbox{$\mathbf L$}} \def\Bm{\UPS}
\def\CC{\mbox{$\mathbf C$}}	\def\cc{\mbox{$\mathbf c$}}
\def\Ca{\Delta} \def\Cb{\GAM}
\def\DD{\mbox{$\mathbf D$}}	\def\dd{\mbox{$\mathbf d$}}
\def\EE{\mbox{$\mathbf E$}}	\def\ee{\mbox{$\mathbf e$}}
\def\E{\,\textup{\textrm{E}}}	
\def\EXy{\,\textup{\textrm{E}}_{\text{{\bf XY}}}}
\def\FF{\mbox{$\mathbf F$}} \def\ff{\mbox{$\mathbf f$}}
\def\GG{\mbox{$\mathbf G$}}	\def\gg{\mbox{$\mathbf g$}}
\def\HH{\mbox{$\mathbf H$}}	\def\hh{\mbox{$\mathbf h$}}
\def\II{\mbox{$\mathbf I$}} \def\ii{\mbox{$\mathbf i$}}
\def\IIm{\mbox{$\mathbf I$}}
\def\JJ{\mbox{$\mathbf J$}}
\def\KK{\mbox{$\mathbf K$}}
\def\LL{\mbox{$\mathbf L$}}	\def\ll{\mbox{$\mathbf l$}}
\def\MM{\mbox{$\mathbf M$}}  \def\mm{\mbox{$\mathbf m$}}
\def\N{\,\textup{\textrm{N}}}
\def\MVN{\,\textup{\textrm{MVN}}}
\def\OO{\mbox{$\mathbf O$}}
\def\PP{\mbox{$\mathbf P$}}  \def\pp{\mbox{$\mathbf p$}}
\def\QQ{\mbox{$\mathbf Q$}}	 \def\qq{\mbox{$\mathbf q$}} \def\Qb{\mbox{$\mathbf G$}}  \def\Qm{\mathbb{Q}}
\def\RR{\mbox{$\mathbf R$}}	 \def\rr{\mbox{$\mathbf r$}} \def\Rb{\mbox{$\mathbf H$}}	\def\Rm{\mathbb{R}}
\def\Ss{\mbox{$\mathbf S$}}
\def\UU{\mbox{$\mathbf U$}}	\def\uu{\mbox{$\mathbf u$}}
\def\Ub{\mbox{$\mathbf C$}} \def\Ua{\mbox{$\mathbf c$}} \def\Um{\UPS}
%\def\VV{\mbox{$\mathbf V$}}	\def\vv{\mbox{$\mathbf v$}}
\def\VV{\mbox{$\pmb{V}$}}	\def\vv{\mbox{$\pmb{v}$}}
%\def\WW{\mbox{$\mathbf W$}}	\def\ww{\mbox{$\mathbf w$}}
\def\WW{\mbox{$\pmb{W}$}}	\def\ww{\mbox{$\pmb{w}$}}
%\def\XX{\mbox{$\mathbf X$}}
\def\XX{\mbox{$\pmb{X}$}}	\def\xx{\mbox{$\pmb{x}$}}
%\def\xx{\mbox{$\mathbf x$}}
%\def\YY{\mbox{$\mathbf Y$}}
\def\YY{\mbox{$\pmb{Y}$}}	\def\yy{\mbox{$\pmb{y}$}}
%\def\yy{\mbox{$\mathbf y$}}
\def\ZZ{\mbox{$\mathbf Z$}}	\def\zz{\mbox{$\mathbf z$}}	\def\Zb{\mbox{$\mathbf M$}} \def\Za{\mbox{$\mathbf N$}} \def\Zm{\XI}
\def\zer{\mbox{\boldmath $0$}}
\def\chol{\,\textup{\textrm{chol}}}
\def\vec{\,\textup{\textrm{vec}}}
\def\var{\,\textup{\textrm{var}}}
\def\cov{\,\textup{\textrm{cov}}}
\def\diag{\,\textup{\textrm{diag}}}
\def\trace{\,\textup{\textrm{trace}}}
\def\hatxt{\widetilde{\xx}_t^T}
\def\hatxttm{\widetilde{\xx}_t^{t-1}}
\def\hatxone{\widetilde{\mbox{$\xx$}}_1}
\def\hatxzero{\widetilde{\mbox{$\xx$}}_0}
\def\hatxtm{\widetilde{\mbox{$\xx$}}_{t-1}^T}
\def\hatxtmtm{\widetilde{\mbox{$\xx$}}_{t-1}^{t-1}}
\def\hatxQtm{\widetilde{\mathbb{x}}_{t-1}}
\def\hatyt{\widetilde{\mbox{$\yy$}}_t}
\def\hatyyt{\widetilde{\mbox{$\mathbf y$}\mbox{$\mathbf y$}^\top}_t}
\def\hatyone{\widetilde{\mbox{$\yy$}}_1}
\def\hatOt{\widetilde{\OO}_t}
\def\hatWt{\widetilde{\WW}_t}
\def\hatwt{\widetilde{\mbox{$\ww$}}_t}
\def\hatYXt{\widetilde{\mbox{$\yy\xx$}}_t}
\def\hatXYt{\widetilde{\mbox{$\xx\yy$}}_t}
\def\hatYXttm{\widetilde{\mbox{$\yy\xx$}}_{t,t-1}}
\def\hatPt{\widetilde{\PP}_t}
\def\hatPtm{\widetilde{\PP}_{t-1}}
\def\hatPQtm{\widetilde{\mathbb{P}}_{t-1}}
\def\hatPttm{\widetilde{\PP}_{t,t-1}}
\def\hatPQttm{\widetilde{\mathbb{P}}_{t,t-1}}
\def\hatPtmt{\widetilde{\PP}_{t-1,t}}
\def\hatVt{\widetilde{\VV}_t^T}
\def\hatVtT{\widetilde{\VV}_t^T}
\def\hatVtt1{\widetilde{\VV}_t^{t-1}}
\def\hatVtm{\widetilde{\VV}_{t-1}^T}
\def\hatVtmtm{\widetilde{\VV}_{t-1}^{t-1}}
\def\hatVttm{\widetilde{\VV}_{t,t-1}^T}
\def\hatVttmtm{\widetilde{\VV}_{t,t-1}^{t-1}}
\def\hatUt{\widetilde{\UU}_t^T}
\def\hatSt{\widetilde{\Ss}_t^T}
\def\hatSttm{\widetilde{\Ss}_t^{t-1}}
\def\hatSttm{\widetilde{\Ss}_{t,t-1}^T}
\def\hatSttmtm{\widetilde{\Ss}_{t,t-1}^{t-1}}
\def\hatSttp{\widetilde{\Ss}_{t,t+1}^T}
\def\hatSttptm{\widetilde{\Ss}_{t,t+1}^{t-1}}
\def\hatBmt{\widetilde{\Bm}_t}
\def\hatCat{\widetilde{\Ca}_t}
\def\hatCbt{\widetilde{\Cb}_t}
\def\hatZmt{\widetilde{\Zm}_t}
\def\YYr{\dot{\mbox{$\pmb{Y}$}}}
\def\yyr{\dot{\mbox{$\pmb{y}$}}}
\def\aar{\dot{\mbox{$\mathbf a$}}}
\def\ZZr{\dot{\mbox{$\mathbf Z$}}}
\def\RRr{\dot{\mbox{$\mathbf R$}}}
\def\IR{\nabla}
\usepackage[round]{natbib} % to get references that are like in ecology papers
% \citet{} for inline citation name (year); \citep for citation in parens (name year)

%allow lines in matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\setcounter{tocdepth}{1} %no subsections in toc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{Sweave}
\begin{document}
\input{Residuals-concordance}
\author{E. E. Holmes\footnote{Northwest Fisheries Science Center, NOAA Fisheries, Seattle, WA 98112, 
       eli.holmes@noaa.gov, http://faculty.washington.edu/eeholmes}}
\title{Computation of Standardized Residuals for MARSS Models}
\maketitle
\begin{abstract}
This report shows how to compute the variance of the joint conditional model and state residuals for multivariate autoregressive Gaussian state-space (MARSS) models. The bulk of the report focuses on `smoothations', which are the residuals conditioned on all the data $t=1$ to $T$. The final part of the report covers `innovations', which are residuals conditioned on the data $t=1$ to $t-1$.  

The MARSS model can be written: $\xx_t=\BB\xx_{t-1}+\uu+\ww_t$, $\yy_t=\ZZ\xx_t+\zz+\vv_t$, where $\ww_t$ and $\vv_t$ are multivariate Gaussian error-terms with variance-covariance matrices $\QQ$ and $\RR$ respectively. The joint conditional residuals are the $\ww_t$ and $\vv_t$ conditioned on the observed data, which may be incomplete (missing values). Harvey, Koopman and Penzer (1998) show a recursive algorithm for the smoothation residuals (conditioned on all the data). I show an alternate algorithm to compute these residuals using the conditional variances of the states and the conditional covariance between unobserved data and states. This allows one to compute the variance of un-observed residuals (residuals associated with missing or left-out data), which is needed for leave-one-out cross-validation tests. I show how to modify the Harvey et al. algorithm in the case of missing values and how to modify it to return the non-normalized conditional residuals.
\end{abstract}
Keywords: Time-series analysis, Kalman filter, residuals, maximum-likelihood, vector autoregressive model, dynamic linear model, parameter estimation, state-space
\vfill
{\noindent \small citation: Holmes, E. E. 2014. Computation of standardized residuals for (MARSS) models. Technical Report. arXiv:1411.0045 }
 \newpage
 
\section{Overview}

This report discusses the computation of the variance of the conditional model and state residuals for MARSS models of the  form:
\begin{equation}\label{eq:residsMARSS}
\begin{gathered}
\xx_t = \BB_t\xx_{t-1} + \uu_t + \ww_t, \text{ where } \WW_t \sim \MVN(0,\QQ_t)\\
\yy_t = \ZZ_t\xx_t + \aa_t + \vv_t, \text{ where } \VV_t \sim \MVN(0,\RR_t)\\
\XX_0 \sim \MVN(\xixi,\LAM)
\end{gathered}
\end{equation}
The state and model residuals are respectively
\begin{equation}\label{eq:resids}
\begin{gathered}
\ww_t = \xx_t - \BB_t\xx_{t-1} - \uu_t\\
\vv_t = \yy_t - \ZZ_t\xx_t - \aa_t
\end{gathered}
\end{equation}

Given a set of observed data $\yy_t$ and states $\xx_t$, the model residuals are $\yy_t - (\ZZ_t \xx_t + \aa_t)=\vv_t$.  The model residual is a random variable since $\yy_t$ and $\xx_t$ are drawn from the joint multivariate distribution of $\YY_t$ and $\XX_t$ defined by the MARSS equations (Equation \ref{eq:residsMARSS}).
The unconditional\footnote{meaning not conditioning on any particular set of observed data but rather taking the expectation across all possible values of $\yy_t$ and $\xx_t$.} variance of the model residuals is
\begin{equation}\label{eqn:unconditiondistofVt}
\var_{XY_t}[\VV_t] = \var_{XY_t}[\YY_t - (\ZZ_t \XX_t + \aa_t)] = \RR_t\\
\end{equation}
based on the distribution of $\VV_t$ in Equation \ref{eq:residsMARSS}.  $\var_{XY_t}$ indicates that the integration is over the joint unconditional distribution of $\XX_t$ and $\YY_t$. 

Once we have data, $\RR_t$ is not the variance-covariance matrix of our model residuals because our residuals are now conditioned\footnote{`conditioned' means that the probability distribution of the residual has changed. The distribution is now the distribution given that $\YY=\yy$, say. Expectations and variances $\var[ ]$ are integrals over the value that a random variable might take multiplied by the probability of that value. When presenting an   `expectation', the probability distribution is normally implicit but for derivations involving conditional expectiations, it is important to be explicit about the distribution that is being integrated over.} on a set of observed data. There are two types of conditional model residuals used in MARSS analyses: innovations and smoothations.  Innovations are the model residuals at time $t$ using the expected value of $\XX_t$ conditioned on the data from 1 to $t-1$.  Smoothations  are the model residuals using the expected value of $\XX_t$ conditioned on all the data, $t=1$ to $T$.  Smoothations are used in computing standardized residuals for outlier and structural break detection \citep{Harveyetal1998, deJongPenzer1998, CommandeurKoopman2007}.  

\section{Distribution of MARSS smoothation residuals}\label{sec:smoothations}

This section discusses computation of the variance of the model and state residuals conditioned on all the data from $t=1$ to $T$.  These MARSS residuals are often used for outlier detection and shock detection, and in this case you only need the distribution of the model residuals for the observed values.  However if you wanted to do a leave-one-out cross-validation, you would need to know the distribution of the residuals for data points you left out (treated as unobserved).  The equations in this report give you the former and the latter, while the algorithm by \citet{Harveyetal1998} gives only the former.

\subsection{Notation and relations}

Throughout, I follow the convention that capital letters are random variables and small letters are a realization from the random variable.  This only applies to random variables; parameters are not random variables\footnote{in a frequentist framework}. Parameters are shown in Roman font while while random variables are bold slanted font. Parameters written as capital letters are matrices, while parameters written in small letters are strictly column matrices. 

In this report, the distribution over which the integration is done in an expectation or variance  is given by the subscript, e.g. $\E_A[f(A)]$ indicates an unconditional expectation over the distribution of $A$ without conditioning on another random variable while $\E_{A|b}[f(A)|b]$ would indicate an expectation over the distribution of $A$ conditioned on $B=b$; presumably $A$ and $B$ are not independent otherwise $B=b$ would have no effect on $A$. $\E_{A|b}[f(A)|b]$ is a fixed value, not random. It is the expected value when $B=b$. In contrast, $\E_{A|B}[f(A)|B]$ denotes the random variable over all the possible $\E_{A|b}[f(A)|b]$ given all the possible $b$ values that $B$ might take. The variance of $\E_{A|B}[f(A)|B]$ is the variance of this random variable. The variance of $\E_{A|b}[f(A)|b]$ in contrast is 0 since it is a fixed value.  We will often be working with the random variables, $\E_{A|B}[f(A)|B]$ or $\var_{A|B}[f(A)|B]$, inside an expectation or variance: such as $\var_B[\E_{A|B}[f(A)|B]]$

\subsubsection{Law of total variance}

The ``law of total variance'' can be written
\begin{equation}\label{eq:lawoftotvar}
\var_A[A] = \var_B[\E_{A|B}[A|B]] + \E_B[\var_{A|B}[A|B]]
\end{equation}
The subscripts on the inner expectations make it explicit that the expectations are being taken over the conditional distributions.  $\var_{A|B}[A|B]$ and $\E_{A|B}[A|B]$ are random variables because the $B$ in the conditional is a random variable. We take the expectation or variance with it fixed at one value, $b$, but $B$ can take other values of $b$ also.  Going forward, I will write this more succinctly as
\begin{equation}
\var[A] = \var_B[\E[A|B]] + \E_B[\var[A|B]]
\end{equation}
It is important to note that $\E_{A|b}[A|b]$ and $\var_{A|b}[A|b]$ are fixed values while $\E_{A|B}[A|B]$ and $\var_{A|B}[A|B]$ are random variables. When computing $\var[\E_{A|B}[A|B]]$, we will typically compute $\E_{A|b}[A|b]$ and then compute (or infer) the variance or expectation of that over all possible values of $b$. 

From the law of total variance will appear in this report in the following form:
\begin{equation}
\var_{XY_t}[f(\YY_t,\XX_t)] = \var_{Y^{(1)}}[\E_{XY_t|Y^{(1)}}[f(\YY_t,\XX_t)|\YY^{(1)}]] + \E_{Y^{(1)}}[\var_{XY_t|Y^{(1)}}[f(\YY_t,\XX_t)|\YY^{(1)}]]
\end{equation}
where $f(\YY_t,\XX_t)$ is some function of $\XX_t$ and $\YY_t$ and $\YY^{(1)}$ is the observed data from $t=1$ to $T$. 


% A joint expectation of $f(\AA,\BB)$ would be $\E_{AB}[f(\AA,\BB)]$, where 'AB' denotes the joint probability distribution. This can be written in terms of the conditional expectation using the law of total expectation $\E_B[\E_{A|b}[f(\AA,\BB)|\BB=\bb]]$.

\subsection{Model residuals conditioned on all the data}

Define the smoothations $\hat{\vv}_t$ as:
\begin{equation}\label{eq:vtT}
\hat{\vv}_t = \yy_t - \ZZ_t\hatxt - \aa_t,
\end{equation}
where  $\hatxt$ is $\E[\XX_t|\yy^{(1)}]$. The smoothation is different from $\vv_t$ because it uses $\hatxt$ not $\xx_t$; $\xx_t$ is not known, $\hatxt$ is its estimate. $\hatxt$ is output by the Kalman smoother. $\yy^{(1)}$ means all the observed data from $t=1$ to $T$. $\yy^{(1)}$ is a sample from the random variable $\YY^{(1)}$. The unobserved $\yy$ will be termed $\yy^{(2)}$ and is a sample from the random variable $\YY^{(2)}$. When $\YY$ appears without a superscript, it means both $\YY^{(1)}$ and $\YY^{(2)}$ together. Similarly $\yy$ means both $\yy^{(1)}$ and $\yy^{(2)}$ together---the observed data that we use to estimate $\hatxt$ and the unobserved data that we do not use and may or may not know. $\hat{\vv}_t$ exists for both $\yy^{(1)}$ and $\yy^{(2)}$, though we might not know $\yy^{(2)}$ and thus might not know its corresponding $\hat{\vv}_t$. In some cases, however, we do know $\yy^{(2)}$; they are data that we left out of our model fitting, in say a k-fold or leave-one-out cross-validation.

$\hat{\vv}_t$ is sample from the random variable $\hat{\VV}_t$ since $\YY$ is a random variable.  We want to compute the mean and variance of this random variable $\hat{\VV}_t$ over all possibles values that $\XX$ and $\YY$ might take. The mean of $\hat{\VV}_t$ is 0 and we are concerned only with computing the variance:
\begin{equation}\label{eq:var.vtT}
\var[\hat{\VV}_t] = \var_{XY}[\YY_t - \ZZ_t\E[\XX_t|\YY^{(1)}] - \aa_t]
\end{equation}
Notice we have an unconditional variance over $XY$ on the outside (that subscript on $\var$) and a conditional expectation over a specific value of $\YY^{(1)}$ on the inside (in the $\E[\;]$).

From the law of total variance (Equation \ref{eq:lawoftotvar}), we can write the variance of the model residuals as
\begin{equation}\label{eq:varvvtgeneral}
\var[\hat{\VV}_t] = \var_{Y^{(1)}}[\E[\hat{\VV}_t|\YY^{(1)}]] + \E_{Y^{(1)}}[\var[\hat{\VV}_t|\YY^{(1)}]]
\end{equation}

\subsubsection{First term in Equation \ref{eq:varvvtgeneral}}

Notice that the random variable inside the $\var[\;]$ in the first term is
\begin{equation}
\E[\hat{\VV}_t|\YY^{(1)}]= \E[(\YY_t + \ZZ_t \E[\XX_t|\YY^{(1)}] + \aa_t)|\YY^{(1)}] 
\end{equation}
Let's consider this for a specific value $\YY^{(1)}=\yy^{(1)}$. In the subscript the $\E$, I will put the random variable's probability distribution that is being integrated over. The only random variable in $\hat{V}_t|\yy^{(1)}$ is $\YY_t^{(2)}$. $\YY^{(1)}$ is fixed and $\XX_t$ is integrated out in the expectation $\E[\XX_t|\yy^{(1)}]$.
\begin{equation}
\E[\hat{\VV}_t|\yy^{(1)}]= \E[(\YY_t + \ZZ_t \E[\XX_t|\yy^{(1)}] + \aa_t)|\yy^{(1)}] =
\E[\YY_t|\yy^{(1)}] + \ZZ_t \E[\E_{X_t|y^{(1)}}[\XX_t|\yy^{(1)}]|\yy^{(1)}] + \E[\aa_t|\yy^{(1)}]
\end{equation}
$\E[\XX_t|\yy^{(1)}]$ is a fixed value, and the expected value of a fixed value is itself. So
$\E[\E[\XX_t|\yy^{(1)}]|\yy^{(1)}]=\E[\XX_t|\yy^{(1)}]$. 
Thus,
\begin{equation}
\E[\hat{\VV}_t|\yy^{(1)}] = \E[\YY_t|\yy^{(1)}] + \ZZ_t \E[\XX_t|\yy^{(1)}] + \E[\aa_t|\yy^{(1)}]
\end{equation}
We can move the conditional out and write
\begin{equation}
\E[\hat{\VV}_t|\yy^{(1)}]= \E[(\YY_t + \ZZ_t \XX_t + \aa_t)|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}].
\end{equation}
The right side is $\E[\VV_t|\yy^{(1)}]$, no `hat' on the $\VV_t$ and this applies for all $\yy^{(1)}$. 
 This means that the first term in Equation \ref{eq:varvvtgeneral} can be written with no hat on $\VV$:
\begin{equation}\label{eq:no.hat.on.V}
\var_{Y^{(1)}}[\E[\hat{\VV}_t|\YY^{(1)}]] = \var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]]
\end{equation}

Using the law of total variance, we can re-write $\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]]$ as:
\begin{equation}\label{eqn:varianceVt}
\var[\VV_t] = \var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]] + \E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]].
\end{equation}
From Equation \ref{eqn:varianceVt}, we can solve for $\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]]$:
\begin{equation}\label{eq:var.E.vtT}
\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]] = \var[\VV_t] - \E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]]
\end{equation}
From Equation \ref{eqn:unconditiondistofVt}, we know that $\var[\VV_t]=\RR_t$ (this is the unconditional variance).  

The second term in Equation \ref{eq:var.E.vtT} to the right of the equal sign and inside the expectation, $\var[\VV_t|\YY^{(1)}]$, is the variance of $\VV_t$ with $\YY^{(1)}$ held at a specific fixed $\yy^{(1)}$. The variability in $\var[\VV_t|\yy^{(1)}]$ (notice $\yy^{(1)}$ not $\YY^{(1)}$ now) comes from $\XX_t$ (and the rest of the $\XX$) and $\YY^{(2)}$ which are random variables. Let's compute this variance for a specific $\yy^{(1)}$ value.
\begin{equation}\label{eqn:varvtcondy}
\var[\VV_t|\yy^{(1)}] = \var[ \YY_t - \ZZ_t\XX_t-\aa_t | \yy^{(1)} ].
\end{equation}
Notice that there is no $\E$ (expectation) on the $\XX_t$; this is $\VV_t$ not $\hat{\VV}_t$. $\aa_t$ is a fixed value and can be dropped; the $\YY_t$ and $\XX_t$ are random variables.

Equation \ref{eqn:varvtcondy} can then be written as\footnote{$\var(A+B)=\var(A)+\var(B)+\cov(A,B)+\cov(B,A)$}:
\begin{equation}\label{eq:var.Vt.yy}
\begin{split}
\var[\VV_t|\yy^{(1)}] &= \var[ \YY_t - \ZZ_t\XX_t | \yy^{(1)} ]\\
&=\var[ - \ZZ_t\XX_t | \yy^{(1)} ] + \var[ \YY_t|\yy^{(1)}] + \cov[ \YY_t, - \ZZ_t\XX_t | \yy^{(1)} ] + \cov[ - \ZZ_t\XX_t, \YY_t | \yy^{(1)} ]\\
&=\ZZ_t \hatVt \ZZ_t^\top + \hatUt - \hatSt\ZZ_t^\top - \ZZ_t(\hatSt)^\top
\end{split}
\end{equation}
If there were no missing data, i.e. if $\yy^{(1)}=\yy$, then $\hatUt$ and $\hatSt$ would be zero and this would reduce to $\ZZ_t \hatVt \ZZ_t^\top$. But we are concerned with the case where there are missing values. Those missing values need not be for all $t$. That is, there may be some observed $y$ at time t and some missing $y$. $\yy_t$ is multivariate.

$\hatVt = \var[ \XX_t | \yy^{(1)} ]$ and is output by the Kalman smoother. $\hatUt=\var[\YY_t|\yy^{(1)}]$ and $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$. The equations for these are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package\footnote{$\hatUt$ is  \texttt{OtT - tcrossprod(ytT)} in the \texttt{MARSShatyt()} output.}.
So we now know $\var[\VV_t|\yy^{(1)}]$ for a specific $\yy^{(1)}$. We want $\E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]]$ which is its expected value of all possible values of $\yy^{(1)}$. $\hatVt$, $\hatUt$ and $\hatSt$ are multivariate normal random variables. The conditional variance of a multivariate normal does not depend on the value that you are conditioning on\footnote{Let the $\AA$ be a N-dimensional multivariate normal random variable partitioned into $\AA_1$ and $\AA_2$ with variance-covariance matrix $\Sigma = \begin{bmatrix}
\Sigma_1 & \Sigma_{12} \\
\Sigma_21 & \Sigma_{2}
\end{bmatrix}$.  The variance-covariance matrix of $\AA$ conditioned on $\AA_1=\aa$ is $\Sigma = \begin{bmatrix}
0 & 0 \\
0 & \Sigma_2 - \Sigma_{12}\Sigma_{1}\Sigma_{21}
\end{bmatrix}$. Notice that $\aa$ does not appear in the conditional variance matrix.}. This means that $\hatVt$, $\hatUt$ and $\hatSt$ do not depend on $\yy^{(1)}$. They only depend on the MARSS model parameters.

Because $\hatVt$, $\hatUt$ and $\hatSt$ only depend on the MARSS parameters values, $\QQ$, $\BB$, $\RR$, etc., the second term in Equation \ref{eq:var.E.vtT}, $\E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]]$, is equal to $\var[\VV_t|\yy^{(1)}]$ (Equation \ref{eq:var.Vt.yy}). Putting this into Equation \ref{eq:var.E.vtT}, we have
\begin{equation}\label{eqn:conditionalvtfinala}
\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)})]]  = \var[\VV_t] - \var[\VV_t|\yy^{(1)}] = \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t(\hatSt)^\top
\end{equation}
Since $\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)})]] = \var_{Y^{(1)}}[\E[\hat{\VV}_t|\YY^{(1)})]]$ (Equation \ref{eq:no.hat.on.V}), this means that the first term in Equation \ref{eq:varvvtgeneral} is
\begin{equation}\label{eqn:conditionalvtfinal}
\var_{Y^{(1)}}[\E[\hat{\VV}_t|\YY^{(1)})]]  =  \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t(\hatSt)^\top
\end{equation}

\subsubsection{Second term in Equation \ref{eq:varvvtgeneral}}

Consider the second term in Equation \ref{eq:varvvtgeneral}.  This term is 
\begin{equation}
\E_{Y^{(1)}}[\var[(\YY_t-\ZZ_t\E[\XX_t|\YY^{(1)}]-\aa_t)|\YY^{(1)}]]
\end{equation}
The middle term is a conditional expectation inside a conditional variance inside an unconditional expectation:
\begin{equation}
\E_{Y^{(1)}}[\var[\E[\XX_t|\YY^{(1)}]|\YY^{(1)}]]
\end{equation}

Let's solve the inner part for a specific $\YY^{(1)}=\yy^{(1)}$. $\E[\XX_t|\yy^{(1)}]$ is a fixed value. Thus  $\var[\E[\XX_t|\yy^{(1)}]|\yy^{(1)}]=0$. The variance of a fixed value is 0. This is true for all $\yy^{(1)}$ so the middle term reduces to 0. $\aa_t$ is also fixed and its variance is also 0. Thus the second term reduces to $\E_{Y^{(1)}}[\var[\YY|\YY^{(1)}]] = \hatUt$. As noted in the previous section, $\hatUt$ is only a function of the MARSS parameters; it is not a function of $\yy^{(1)}$. Thus the second term in Equation \ref{eq:varvvtgeneral} is simply $\hatUt$.

\subsubsection{Putting together the first and second terms}
We can now put the first and second terms in Equation \ref{eq:varvvtgeneral} together and write out the equation for the variance of the model residuals:
\begin{equation}
\begin{split}
\var[\hat{\VV}_t] &= \RR_t - \ZZ_t \hatVt \ZZ_t^\top - \hatUt + \hatSt\ZZ_t^\top + \ZZ_t(\hatSt)^\top + \hatUt\\
&= \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t(\hatSt)^\top
\end{split}
\end{equation}
This will reduce to $\RR_t - \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ has no missing values and to $\RR_t + \ZZ_t \hatVt \ZZ_t^\top$ if $\yy_t$ is all missing values. The behavior if $\yy_t$ has some missing and some not missing values depends on whether $\RR_t$ is a diagonal matrix or not (i.e. if the $\yy_t^{(1)}$ and $\yy_t^{(2)}$ are correlated).

\subsection{State residuals conditioned on the data}

The state residuals are $\xx_t - (\BB_t \xx_{t-1} + \uu_t)=\ww_t$.  The unconditional expected value of the state residuals is $\E[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \E[\WW_t] = 0$ and the unconditional variance of the state residuals is
\begin{equation}
\var[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \var[\WW_t] = \QQ_t
\end{equation}
based on the definition of $\WW_t$.
The conditional state residuals (conditioned on the data from $t=1$ to $t=T$) are defined as
\begin{equation}
\hat{\ww}_t = \hatxt - \BB_t\hatxtm - \uu_t.
\end{equation}
where $\hatxt=E[\XX_t|\yy^{(1)}]$ and $\hatxtm=E[\XX_{t-1}|\yy^{(1)}]$.  $\hat{\ww}_t$ is a sample from the random variable $\hat{\WW}_t$; random over different possible data sets.  The expected value of $\hat{\WW}_t$ is 0, and we are concerned with computing its variance.

We can write the variance of $\WW_t$ (no hat) using the law of total variance.
\begin{equation}\label{eq:Wlawoftotvar}
\var[\WW_t] = \var_{Y^{(1)}}[\E[\WW_t|\YY^{(1)}]] + \E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]]
\end{equation}
Notice that $\var_{Y^{(1)}}[\E[\WW_t|\YY^{(1)}]] = \var[\hat{\WW}_t]$.
\begin{equation}
\E[\WW_t|\yy^{(1)}] = \E[(\XX_t - \BB_t \XX_{t-1} - \uu_t)|\yy^{(1)}] =  \hatxt - \BB_t \hatxtm - \uu_t = \E[\hat{\WW}_t|\yy^{(1)}] = \hat{\ww}_t
\end{equation}
Thus the random variable $\E[\WW_t|\YY^{(1)}$ is $\hat{\WW}_t$, and Equation \ref{eq:Wlawoftotvar} becomes
\begin{equation}
\var[\WW_t] = \var[\hat{\WW}_t] + \E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]]
\end{equation}
Solve for $\var[\hat{\WW}_t]$:
\begin{equation}\label{eqn:varwwt}
\var[\hat{\WW}_t] = \var[\WW_t] - \E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]]
\end{equation}

The variance in the expectation on the far right for a specific $\YY^{(1)}=\yy^{(1)}$ is
\begin{equation}
\var[\WW_t|\yy^{(1)}] = \var[ (\XX_t - \BB_t\XX_{t-1}-\uu_t) | \yy^{(1)} ]
\end{equation}
$\uu_t$  is not a random variable and can be dropped. Thus,
\begin{equation}\label{eq:var.W.cond.y1}
\begin{split}
\var[\WW_t&|\yy^{(1)}] = \var[ (\XX_t - \BB_t\XX_{t-1}) | \yy^{(1)} ] \\
& = \var[ \XX_t | \yy^{(1)} ] + \var[\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[\XX_t, -\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[ -\BB_t\XX_{t-1}, \XX_t | \yy^{(1)} ]\\
& = \hatVt + \BB_t \hatVtm \BB_t^\top - \hatVttm \BB_t^\top - \BB_t\widetilde{\VV}_{t-1,t}
\end{split}
\end{equation}
Again this is conditional multivariate normal variance does not depend on the actual value, $\yy^{(1)}$ that we are conditioning on.  It depends only on the parameters values, $\QQ$, $\BB$, $\RR$, etc., and is the same for all values of $\yy^{(1)}$. So $\E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]] = \var[\WW_t|\yy^{(1)}]$, using any value of $\yy^{(1)}$.

Putting $\E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]]$ from Equation \ref{eq:var.W.cond.y1} and $\var[\WW_t]=\QQ_t$ into Equation \ref{eqn:varwwt}, the variance of the conditional state residuals is
\begin{equation}
\var[\hat{\WW}_t] = \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t}
\end{equation}

\subsection{Covariance of the conditional model and state residuals}
The unconditional model and state residuals, $\VV_t$ and $\WW_t$, are independent by definition\footnote{This independence is specific to the way I have written the MARSS model. It is possible for the model and state residuals to covary. In the MARSS model written in \citet{Harveyetal1998} form, they do covary.} (in Equation \ref{eq:residsMARSS}), i.e. $\cov[\VV_t,\WW_t]=0$.  However the conditional model and state residuals, $\cov[\hat{\VV}_t,\hat{\WW}_t]$, are not independent since both depend on $\yy^{(1)}$.  
Using the law of total covariance, we can write
\begin{equation}\label{eqn:covhatVtWt1}
\cov[\hat{\VV}_t,\hat{\WW}_t] = 
\cov_{Y^{(1)}}[\E[\hat{\VV}_t|\YY^{(1)}],\E[\hat{\WW}_t|\YY^{(1)}]] + \E_{Y^{(1)}}[\cov[\hat{\VV}_t, \hat{\WW}_t|\YY^{(1)}]]
\end{equation}

For a specific value of $\YY^{(1)}=\yy^{(1)}$, the covariance in the second term on the right is $\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]$. Conditioned on a specific value of $\YY^{(1)}$, $\hat{\WW}_t$ is a fixed value. $\hat{\ww}_t = \hatxt - \BB_t\hatxtm - \uu_t$ and,  conditioned on $\yy^{(1)}$, $\hatxt$ and $\hatxtm$ are fixed values. $\uu_t$ is also fixed, because it is a parameter. $\hat{\VV}_t$ is not a fixed value because it has $\YY_t^{(2)}$ and that is a random variable.  So $\cov[\hat{\VV}_t, \hat{\WW}_t|\yy^{(1)}]$ is the covariance between a random variable and a fixed variable and that covariance is 0.

This eliminates the second right-side term in Equation \ref{eqn:covhatVtWt1} and it reduces to
\begin{equation}\label{eqn:covhatVtWt3}
\cov[\hat{\VV}_t,\hat{\WW}_t] = \cov_{Y^{(1)}}[\E[\hat{\VV}_t|\YY^{(1)}],\E[\hat{\WW}_t|\YY^{(1)}]]
\end{equation}
The right side of Equation \ref{eqn:covhatVtWt3} can be written in terms of $\VV_t$ and $\WW_t$ instead of $\hat{\VV}_t$ and $\hat{\WW}_t$:
\begin{equation}\label{eqn:covhatVtWt2}
\cov[\hat{\VV}_t,\hat{\WW}_t] = \cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_t|\YY^{(1)}]]
\end{equation}
since\footnote{$\E[\WW_t|\yy^{(1)}]= \E[\XX_t|\yy^{(1)}]-\BB_t\E[\XX_{t-1}|\yy^{(1)}] - \uu_t = \hatxt-\BB_t \hatxtm - \uu_t = \hat{\ww}_t = \E[\hat{\WW_t}|\yy^{(1)}]$ and 
$\E[\VV_t|\yy^{(1)}]= \E[\YY_t|\yy^{(1)}]-\ZZ_t\E[\XX_{t}|\yy^{(1)}] - \aa_t = \E[\YY_t|\yy^{(1)}]-\ZZ_t \hatxt - \aa_t = \E[\hat{\VV_t}|\yy^{(1)}]$} $\E[\hat{\WW_t}|\yy^{(1)}]=\E[\WW_t|\yy^{(1)}]$ and $\E[\hat{\VV_t}|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}]$.

In the same way we used the law of total variance, we can use the law of total covariance  to obtain $\cov[\hat{\VV}_t,\hat{\WW}_t]$, we can use it to obtain $\cov[\VV_t, \WW_t]$:
\begin{equation}\label{eqn:covVtWt}
\cov[\VV_t, \WW_t] = \E_{Y^{(1)}}[\cov[\VV_t, \WW_t|\yy^{(1)}]] + \cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]]\\ 
\end{equation}
The unconditional covariance of $\VV_t$ and $\WW_t$ is 0. Thus the left side of Equation \ref{eqn:covhatVtWt3} is 0 and we can rearrange the equation as
\begin{equation}\label{eqn:covVtWt2}
\cov_{Y^{(1)}}[\E[\VV_t|\yy^{(1)}],\E[\WW_t|\yy^{(1)}]] = - \E_{Y^{(1)}}[\cov[\VV_t, \WW_t|\yy^{(1)}]]
\end{equation}
Combining Equation \ref{eqn:covhatVtWt2} and \ref{eqn:covVtWt2}, we get
\begin{equation}\label{eqn:conditionalcovvtwt}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\yy^{(1)}] ] 
\end{equation}
and our problem reduces to solving for the conditional covariance of the model and state residuals.  

For a specific $\YY^{(1)}=\yy^{(1)}$, the conditional covariance $\cov[\VV_t, \WW_t|\yy^{(1)}]$ can be written out as
\begin{equation}
\cov[\VV_t, \WW_t|\yy^{(1)}] = \cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_t-\BB_t\XX_{t-1}-\uu_t|\yy^{(1)}]
\end{equation}
$\aa_t$ and $\uu_t$ are fixed values and can be dropped. Thus
\begin{equation}
\begin{split}
\cov&[\VV_t, \WW_t|\yy^{(1)}] =\cov[\YY_t-\ZZ_t\XX_t, \XX_t-\BB_t\XX_{t-1}|\yy^{(1)}] \\
& =\cov[\YY_t,\XX_t|\yy^{(1)}] + \cov[\YY_t,-\BB_t\XX_{t-1}|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,\XX_t|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,-\BB_t\XX_{t-1}|\yy^{(1)}]\\
& = \hatSt - \hatSttm\BB_t^\top - \ZZ_t\hatVt + \ZZ_t\hatVttm\BB_t^\top
\end{split}
\end{equation}
where $\hatSt=\cov[\YY_t,\XX_t|\yy^{(1)}]$ and $\hatSttm=\cov[\YY_t,\XX_{t-1}|\yy^{(1)}]$; the equations for $\hatSt$ and $\hatSttm$ are given in \citet{Holmes2010} and are output by the \verb@MARSShatyt@ function in the MARSS R package.

$\hatVt$, $\hatVttm$, $\hatSt$ and $\hatSttm$ are conditional multivariate normal and are only functions of the MARSS parameters not of $\yy^{(1)}$. Thus 
\begin{equation}
\E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\YY^{(1)}] ]= \cov[\VV_t, \WW_t|\yy^{(1)}] = \hatSt - \hatSttm\BB_t^\top + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt
\end{equation}
$\cov[\hat{\VV}_t,\hat{\WW}_t]$ is the negative of this (Equation \ref{eqn:conditionalcovvtwt}), thus
\begin{equation}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \hatSt + \hatSttm\BB_t^\top - \ZZ_t\hatVttm\BB_t^\top + \ZZ_t\hatVt
\end{equation}

The Harvey et al. algorithm (shown below) gives the joint distribution of the model residuals at time $t$ and state residuals at time $t+1$.  Using the law of total covariance as above, the covariance in this case is
\begin{equation}
\cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_{t+1}|\YY^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\YY^{(1)}] ] 
\end{equation}
and
\begin{equation}
\begin{split}
\cov[\VV_t, \WW_{t+1}|\yy^{(1)}] & =\cov[\YY_t-\ZZ_t\XX_t-\aa_t, \XX_{t+1}-\BB_{t+1}\XX_t-\uu_{t+1}|\yy^{(1)}] \\
& =\cov[\YY_t-\ZZ_t\XX_t, \XX_{t+1}-\BB_{t+1}\XX_t|\yy^{(1)}] \\
& = \hatSttp - \hatSt\BB_{t+1}^\top - \ZZ_t\widetilde{\VV}_{t,t+1} + \ZZ_t\hatVt\BB_{t+1}^\top
\end{split}
\end{equation}
Thus,
\begin{equation}
\cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_{t+1}|\YY^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\YY^{(1)}] ] = - \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top.
\end{equation}


\subsection{Joint distribution of the conditional residuals}
We now can write the variance of the joint distribution of the conditional residuals. Define
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_t\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \hatxt - \BB_t\hatxtm-\uu_t \end{bmatrix}.
\end{equation}
where $\hatxt$ and $\hatxtm$ are conditioned on $\yy{(1)}$, the observed $\yy$.
$\hat{\varepsilon}_t$ is a sample drawn from the distribution of $\hat{\mathcal{E}}_t$ conditioned on observations at the $(1)$ locations in $\YY$.  The expected value of $\hat{\mathcal{E}}_t$ over all possible $\yy$ is 0 and the variance of $\hat{\mathcal{E}}_t$  is
\begin{equation}\label{eqn:jointcondresid1general}
 \begin{bmatrix}[c|c]
 \RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top+\ZZ_t(\hatSt)^\top&
 \hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt \\
 \rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
 (\hatSt - \hatSttm\BB_t^\top  + \ZZ_t\hatVttm\BB_t^\top - \ZZ_t\hatVt)^\top& 
 \QQ_t - \hatVt - \BB_t \hatVtm \BB_t^\top + \hatVttm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t} \end{bmatrix}
\end{equation}

If the residuals are defined as in \citet{Harveyetal1998},
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxt-\aa_t\\ \tilde{\xx}_{t+1} - \BB_{t+1}\hatxt-\uu_{t+1} \end{bmatrix}
\end{equation}
and the variance of $\hat{\mathcal{E}}_t$ is
\begin{equation}\label{eqn:jointcondresid2}
\begin{bmatrix}[c|c]
\RR_t - \ZZ_t \hatVt \ZZ_t^\top + \hatSt\ZZ_t^\top + \ZZ_t(\hatSt)^\top&
- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top \\
\rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
(- \hatSttp + \hatSt\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1} - \ZZ_t\hatVt\BB_{t+1}^\top)^\top& 
\QQ_{t+1} - \widetilde{\VV}_{t+1} - \BB_{t+1} \hatVt \BB_{t+1}^\top + \widetilde{\VV}_{t+1,t} \BB_{t+1}^\top + \BB_{t+1}\widetilde{\VV}_{t,t+1} \end{bmatrix}
\end{equation}

The above gives the variance of both `observed' model residuals (the ones associated with $\yy^{(1)}$) and the unobserved model residuals (the ones associated with $\yy^{(2)}$).  
When there are no missing values in $\yy_t$, the $\hatSt$ and $\hatSttm$ terms equal 0 and drop out.

\section{Harvey et al. 1998 algorithm for the conditional residuals}
\citet[pgs 112-113]{Harveyetal1998} give a recursive algorithm for computing the variance of the conditional residuals when the time-varying MARSS equation is written as: 
\begin{equation}\label{eqn:residsMARSSHarvey}
\begin{gathered}
\xx_{t+1} = \BB_{t+1}\xx_t + \uu_{t+1} + \GG_{t+1}\epsilon_{t},\\
\yy_t = \ZZ_t\xx_t + \aa_t + \HH_t\epsilon_t,\\
\mbox{ where } \epsilon_t \sim \MVN(0,\II_{m+n \times m+n}), \GG_t\GG_t^\top=\QQ_t\text{ and }\HH_t\HH_t^\top=\RR_t
\end{gathered}
\end{equation}
$\GG_t$ has $m$  rows and $m+n$ columns with the last $n$ columns all 0; $\HH_t$ has $n$ rows and $m+n$ columns with the last $m$ columns all zero.  The algorithm in \citet{Harveyetal1998} gives the variance of the `normalized' residuals, the $\epsilon_t$.  I have modified their algorithm so it  returns the `non-normalized' residuals:
$$\varepsilon_t=\begin{bmatrix}\HH_t\epsilon_t\\ \GG_{t+1}\epsilon_t\end{bmatrix}=\begin{bmatrix}\vv_t\\ \ww_{t+1} \end{bmatrix}.$$

The Harvey et al. algorithm is a backwards recursion using the following output from the Kalman filter: the one-step ahead prediction covariance $\FF_t$, the Kalman gain $\KK_t$, and $\widetilde{\VV}_t^{t-1}=\var[\XX_t|\yy^{(1,1:{t-1}}]$. These are output from \texttt{MARSSkfss()} in \texttt{Sigma}, \texttt{Kt}, and \texttt{Vtt1}.

\section{Algorithm}

Start from $t=T$ and work backwards to $t=1$. At time $T$, $r_T=0_{1 \times m}$ and $N_T=0_{m \times m}$. $\BB_{t+1}$ and $\QQ_{t+1}$ can be set to NA or 0. They will not appear in the algorithm at time $T$ since $r_T=0$ and $N_T=0$. Note that the $\ww$ residual and its associated variance and covariance with $\vv$ at time $T$ is NA since this residual would be for $\xx_T$ to $\xx_{T+1}$.

% algorithm with \GG_t and \HH_t
% \begin{equation}\label{eqn:Harveyalgo}
% \begin{gathered}
% \FF_t = \ZZ_t\hatVt\ZZ_t^\top+\RR_t\\
% G_t= \GG_t\QQ_t\GG_t^\top, \mbox{  }H_t = \HH_t\RR_t\HH_t^\top,  \mbox{ } K_t = \BB_t\KK^{*}_t\\
% L_t = \BB_t - K_t\ZZ_t, \mbox{ } J_t= H_t - K_t G_t, \mbox{ } u_t = \FF_t^{-1} - K_t^\top r_t\\
% r_{t-1} = \ZZ_t^\top u_t + \BB_t^\top r_t, \mbox{ } N_{t-1} = K_t^\top N_t K_t + L_t^\top N_t L_t
% \end{gathered}
% \end{equation}

\begin{equation}\label{eqn:Harveyalgo}
\begin{gathered}
\QQ^*_{t+1}=\begin{bmatrix}\QQ_{t+1}&0_{m \times n}\end{bmatrix}, \mbox{    } \RR^*_t=\begin{bmatrix}0_{n \times m}&\RR_t^*\end{bmatrix}\\
\FF_t = \ZZ_t^*\widetilde{\VV}_t^{t-1}{\ZZ_t^*}^\top+\RR_t^*\mbox{,   } K_t = \BB_{t+1}\KK_t = \BB_{t+1} \widetilde{\VV}_t^{t-1}{\ZZ_t^*}^\top \FF_t^{-1}  \\
L_t = \BB_{t+1} - K_t\ZZ_t^*, \mbox{    } J_t= \QQ^*_{t+1} - K_t \RR^*_t, \mbox{    } u_t = \FF_t^{-1} \hat{\vv}_t - K_t^\top r_t\\
r_{t-1} = {\ZZ_t^*}^\top u_t + \BB_{t+1}^\top r_t, \mbox{    } N_{t-1} = {\ZZ_t^*}^\top \FF_t^{-1} \ZZ_t^* + L_t^\top N_t L_t 
\end{gathered}
\end{equation}
Bolded terms are the same as in Equation \ref{eqn:residsMARSSHarvey} or output by \texttt{MARSSkfss()}.  Unbolded terms are terms used in \citet{Harveyetal1998}.  The * on $\ZZ_t$ and $\RR_t$, indicates that they are the missing value modified versions  discussed in \citet[section 6.4]{ShumwayStoffer2006} and \citet{Holmes2012}: the rows of $\ZZ_t$ corresponding to missing rows of $\yy_t$ are set to zero and the $(i,j)$ and $(j,i)$ terms of $\RR_t$ corresponding the missing rows of $\yy_t$ are set to zero.  For the latter, this means if the $i$-th row of $\yy_t$ is missing, then then all the $(i,j)$ and $(j,i)$ terms, including $(i,i)$ are set to 0. It is assumed that a missing values modified inverse of $\FF_t$ is used: any 0's on the diagonal of $\FF_t$ are replaced with 1, the inverse is taken, and 1s on diagonals is replaced back with 0s.

The residuals \citep[eqn 24]{Harveyetal1998} are
\begin{equation}\label{eqn:Harveyresiduals}
\hat{\varepsilon}^*_t = \begin{bmatrix}\hat{\vv}_t\\ \hat{\ww}_{t+1}\end{bmatrix} =({\RR^*_t})^\top u_t + ({\QQ^*_{t+1}})^\top r_t
\end{equation}
with mean of 0 ($\E_{Y^{(1)}}(\hat{\varepsilon}_t)=0$) and variance
\begin{equation}\label{eqn:Harveyvariance}
\Sigma_t^* = \var_{Y^{(1)}}(\hat{\varepsilon}_t) ={\RR^*_t}^\top \FF_t^{-1} \RR^*_t + J_t^\top N_t J_t
\end{equation}
The * signifies that these are the missing values modified $\hat{\varepsilon}_t$ and $\Sigma_t$; see comments above.

\subsection{Difference in notation}
In Equation 20 in \citet{Harveyetal1998}, their $T_t$ is my $\BB_{t+1}$ and their $H_t H_t^\top$ is my $\QQ_{t+1}$.  Notice the difference in the time indexing. My time indexing on $\BB$ and $\QQ$ matches the left $\xx$ while in theirs, $T$ and $H$ indexing matches the right $\xx$. Thus in my implementation of their algorithm \citep[eqns. 21-24]{Harveyetal1998}, $\BB_{t+1}$ appears in place of $T_t$ and $\QQ_{t+1}$ appears in place of $H_t$. See comments below on normalization and the difference between $\QQ$ and $H$. 

\citet[eqns. 19, 20]{Harveyetal1998} use $G_t$ to refer to the $\chol(\RR_t)^\top$ (essentially) and $H_t$ to refer to $\chol(\QQ_t)^\top$.  I have replaced these with $\RR_t^*$ and $\QQ_t^*$, respectively, which causes my variant of their algorithm (Equation \ref{eqn:Harveyalgo}) to give the `non-normalized' variance of the residuals. The residuals function in the MARSS package has an option to give either normalized or non-normalized residuals.

$\KK_t$ is the Kalman gain output by the MARSS package.  The Kalman gain as used in the \citet{Harveyetal1998} algorithm is $K_t=\BB_{t+1}\KK_t$. Notice that Equation 21 in \citet{Harveyetal1998} has $H_t G_t^\top$ in the equation for $K_t$ (Kalman gain). This is the covariance of the state and observation errors, which is allowed given the way they write the errors in their Equations 19 and 20. The way the MARSS package model is written, the state and observation errors are independent of each other. Thus $H_t G_t^\top = 0$ and this term drops out of the $\KK_t$ equation.

\subsection{Computing the standardized residuals}
The standardized residuals are computed by multiplying $\hat{\varepsilon}_t$ by the inverse of the square root of the variance-covariance matrix from which $\hat{\varepsilon}_t$ is ``drawn'':
\begin{equation}
(\Sigma_t^*)^{-1/2}\hat{\varepsilon}_t^*
\end{equation}
Notice that the missing values modified $\hat{\varepsilon}_t^*$ and $\Sigma_t^*$ are used. if the $i$-th row of $\yy_t$ is missing, the $i$-th row of $\hat{\varepsilon}_t$ is set to 0 and the $i$-th row and column of $\Sigma_t$ is set to all 0.
There will be 0s on the diagonal of $\Sigma_t^*$ so your code will need to deal with these.

\section{Distribution of the MARSS innovation residuals}

One-step-ahead predictions (innovations) are often shown for MARSS models and these are used for likelihood calculations. Innovations are the prediction of $\mathbf{y}$ given data up to $t-1$. Although not termed 'innovations', we can also compute the predictions of $\mathbf{x}$ conditioned on the data up the $t-1$. This section gives the residual variance for these one-step-ahead predictions. 


\subsubsection{Variance of the one-step-ahead model residuals}

Define the innovations $\tilde{\vv}_t$ as:
\begin{equation}\label{eq:vtT}
\tilde{\vv}_t = \yy_t - \ZZ_t\hatxttm - \aa_t,
\end{equation}
The random variable over all possible $\yy_1^{t-1}$ is $\tilde{\VV}_t$. Its mean is 0 and we want to find its variance.


The derivation of the variance of $\tilde{\VV}_t$ follows the same steps as smoothations, $\hat{\VV}_t$, and we can write the variance as:
\begin{equation}\label{eq:innov.model}
\var[\hat{\VV}_t] = \RR_t - \ZZ_t \hatVttm \ZZ_t^\top + \hatSttm\ZZ_t^\top + \ZZ_t(\hatSttm)^\top
\end{equation}
where the $\hatVttm$ and $\hatSttm$ are now conditioned on only the data from 1 to $t-1$.

$\hatSttm=\cov[\YY_t,\XX_t|\yy^{(1)}_{t-1}]$ which is $\cov[\ZZ_t \XX_t + \aa_t,\XX_t|\yy^{(1)}_{t-1} = \ZZ_t \hatVttm \ZZ_t^\top$. Similarly, $\ZZ_t(\hatSttm)^\top=\ZZ_t \hatVttm \ZZ_t^\top$. Thus Equation \ref{eq:innov.model} reduces to
\begin{equation}\label{eq:innov.model2}
\var[\hat{\VV}_t] = \RR_t + \ZZ_t \hatVttm \ZZ_t^\top
\end{equation}

\subsection{State residuals conditioned on the data}

Define the state residuals conditioned on the data from 1 to $t-1$ as $\tilde{\ww}_t$.
\begin{equation}\label{eq:vtT}
\tilde{\ww}_t = \xx_t - \BB_t\hatxttm - \uu_t,
\end{equation}
The random variable over all possible $\yy_1^{t-1}$ is $\tilde{\WW}_t$. Its mean is 0 and we want to find its variance.

The derivation of the variance of $\tilde{\WW}_t$ follows the same steps as for $\hat{\WW}_t$, except that we are using the variances of $\XX$ conditioned on the data from 1 to $t-1$ instead of all the data.  We can write the variance as:
\begin{equation}
\var[\tilde{\WW}_t]  =  \QQ_t - \hatVttm - \BB_t \hatVtmtm \BB_t^\top + \hatVttmtm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t}^{t-1}
\end{equation}

\subsection{Covariance of the conditional model and state residuals}

\begin{equation}
\cov[\hat{\VV}_t,\hat{\WW}_t] = - \hatSttm + \hatSttmtm\BB_t^\top - \ZZ_t\hatVttmtm\BB_t^\top + \ZZ_t\hatVttm = 0
\end{equation}


\begin{equation}
\cov[\hat{\VV}_t,\hat{\WW}_{t+1}] = - \hatSttptm + \hatSttm\BB_{t+1}^\top  - \ZZ_t\hatVttm\BB_{t+1}^\top + \ZZ_t\widetilde{\VV}_{t,t+1}^{t-1} = 0
\end{equation}

\subsection{Joint distribution of the conditional residuals}
We now the write the variance of the joint distribution of the conditional residuals. Define
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\tilde{\vv}_t\\ \tilde{\ww}_t\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxttm-\aa_t\\ \hatxttm - \BB_t\hatxtmtm-\uu_t \end{bmatrix}.
\end{equation}
where $\hatxttm$ and $\hatxtmtm$ are conditioned on the observed $\yy$ from $t=1$ to $t-1$.

The expected value of $\hat{\mathcal{E}}_t$ over all possible $\yy$ is 0 and the variance of $\hat{\mathcal{E}}_t$  is
\begin{equation}\label{eqn:jointcondresid1innovs}
 \begin{bmatrix}[c|c]
 \RR_t + \ZZ_t \hatVttm \ZZ_t^\top& 0 \\
 \rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
 0 & \QQ_t - \hatVttm - \BB_t \hatVtmtm \BB_t^\top + \hatVttmtm \BB_t^\top + \BB_t\widetilde{\VV}_{t-1,t}^{t-1} \end{bmatrix}
\end{equation}

If the residuals are defined as in \citet{Harveyetal1998},
\begin{equation}
\hat{\varepsilon}_t = \begin{bmatrix}\tilde{\vv}_t\\ \tilde{\ww}_{t+1}\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxttm-\aa_t\\ \tilde{\xx}_{t+1} - \BB_{t+1}\hatxttm-\uu_{t+1} \end{bmatrix}
\end{equation}
and the variance of $\hat{\mathcal{E}}_t$ is
\begin{equation}\label{eqn:jointcondresid2.innov.t1}
\begin{bmatrix}[c|c]
\RR_t + \ZZ_t \hatVttm \ZZ_t^\top& 0 \\
\rule[.5ex]{40ex}{0.25pt} & \rule[.5ex]{50ex}{0.25pt} \\
0 & \QQ_{t+1} - \widetilde{\VV}_{t+1}^{t-1} - \BB_{t+1} \hatVttm \BB_{t+1}^\top + \widetilde{\VV}_{t+1,t}^{t-1} \BB_{t+1}^\top + \BB_{t+1}\widetilde{\VV}_{t,t+1}^{t-1} \end{bmatrix}
\end{equation}


\bibliography{./EMDerivation}
\bibliographystyle{apalike}

\end{document}
